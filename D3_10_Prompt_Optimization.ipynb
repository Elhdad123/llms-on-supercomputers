{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7ac63",
   "metadata": {},
   "source": [
    "# Prompt Optimization with Evidently\n",
    "Attribution & License\n",
    "\n",
    "This notebook is adapted from: [evidentlyai/community-examples](https://github.com/evidentlyai/community-examples.git), licensed under the Apache License, Version 2.0. © Original authors.\n",
    "\n",
    "Modifications: by Simeon Harrison/EuroCC Austria, © 2025.\n",
    "\n",
    "This notebook demonstrates how to use Evidently's `PromptOptimizer` API for optimizing prompts for LLM judges. \n",
    "\n",
    "## Code Review Quality Classifier\n",
    "We'll walk through optimizing a prompt that classifies the quality of code reviews written for junior developers.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to set up a dataset for LLM evaluation\n",
    "- How to define an LLM judge with a prompt template\n",
    "- How to run the prompt optimization loop\n",
    "- How to retrieve and inspect the best performing prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the required packages yet:\n",
    "# !pip install evidently openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc9af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.optimization import PromptOptimizer\n",
    "from evidently.descriptors import HuggingFace, HuggingFaceToxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5f6441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated review</th>\n",
       "      <th>Expert label</th>\n",
       "      <th>Expert comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This implementation appears to work, but the a...</td>\n",
       "      <td>bad</td>\n",
       "      <td>The tone is slighly condescending, no actionab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great job! Keep it up!</td>\n",
       "      <td>bad</td>\n",
       "      <td>Not actionable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It would be advisable to think about modularit...</td>\n",
       "      <td>bad</td>\n",
       "      <td>there is a suggestion, but no real guidance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You’ve structured the class very well, and the...</td>\n",
       "      <td>good</td>\n",
       "      <td>Good tone, actionable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great job! This is clean and well-organized. T...</td>\n",
       "      <td>bad</td>\n",
       "      <td>Pure praise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Generated review Expert label  \\\n",
       "0  This implementation appears to work, but the a...          bad   \n",
       "1                             Great job! Keep it up!          bad   \n",
       "2  It would be advisable to think about modularit...          bad   \n",
       "3  You’ve structured the class very well, and the...         good   \n",
       "4  Great job! This is clean and well-organized. T...          bad   \n",
       "\n",
       "                                      Expert comment  \n",
       "0  The tone is slighly condescending, no actionab...  \n",
       "1                                     Not actionable  \n",
       "2        there is a suggestion, but no real guidance  \n",
       "3                              Good tone, actionable  \n",
       "4                                        Pure praise  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "review_dataset = pd.read_csv(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/code_review_dataset.csv\")\n",
    "review_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e464810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how Evidently should interpret your dataset\n",
    "dd = DataDefinition(\n",
    "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
    "    categorical_columns=[\"Expert label\"],\n",
    "    llm=LLMClassification(input=\"Generated review\", target=\"Expert label\", reasoning=\"Expert comment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3957c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your pandas DataFrame into an Evidently Dataset\n",
    "dataset = Dataset.from_pandas(review_dataset, data_definition=dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d1a5f7-6bbb-4532-870f-5ec970313066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API key (input is hidden):  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key (input is hidden): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af027bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template and judge for classifying code review quality\n",
    "criteria = '''A review is GOOD when it's actionable and constructive.\n",
    "A review is BAD when it is non-actionable or overly critical.'''\n",
    "\n",
    "feedback_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=criteria,\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\n",
    "    alias=\"Code Review Judge\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    column_name=\"Generated review\",\n",
    "    template=feedback_quality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6995309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed prompt 'A review is GOOD when it's actionable an...', got preds(50) preds_reasoning(50)\n",
      "Prompt scored: AccuracyScorer: 0.66\n",
      "Prompt 'A review is GOOD when it's actionable an...' optimized to 'A review is GOOD when it provides constr...'\n",
      "Executed prompt 'A review is GOOD when it provides constr...', got preds(50) preds_reasoning(50)\n",
      "Prompt scored: AccuracyScorer: 0.94\n",
      "Prompt 'A review is GOOD when it provides constr...' optimized to 'A review is GOOD when it provides constr...'\n",
      "Executed prompt 'A review is GOOD when it provides constr...', got preds(50) preds_reasoning(50)\n",
      "Prompt scored: AccuracyScorer: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer and run optimization using feedback strategy\n",
    "optimizer = PromptOptimizer(\"code_review_example\", strategy=\"feedback\")\n",
    "optimizer.set_input_dataset(dataset)\n",
    "await optimizer.arun(judge, \"accuracy\")\n",
    "# for sync version:\n",
    "# optimizer.run(judge, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f3162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A review is GOOD when it provides constructive, actionable feedback that includes specific suggestions for improvement and avoids vague language. A review is BAD when it is overly critical, lacks clarity, or fails to offer concrete steps for enhancement. Consider feedback that maintains a balance between being constructive and encouraging while still addressing areas that need attention.\n"
     ]
    }
   ],
   "source": [
    "# Show the best-performing prompt template found by the optimizer\n",
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2893069-e592-40b3-a1a4-3eac138aca09",
   "metadata": {},
   "source": [
    "## Example 2: Bookings Query Classifier\n",
    "In this tutorial, we'll optimize a prompt for classifying different types of customer service queries (like Booking, Payment, or Technical issues) using an LLM classifier.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to load a dataset for LLM classification\n",
    "- How to define a multiclass classification prompt\n",
    "- How to run prompt optimization with Evidently\n",
    "- How to retrieve the best performing prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5649d2a-9cd0-41b3-82af-ad038c7b584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import MulticlassClassificationPromptTemplate\n",
    "from evidently.llm.optimization import PromptOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c123275-866b-48fb-9de7-c19b1a935159",
   "metadata": {},
   "source": [
    "### Load Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1908e0b-1670-42aa-b57f-0ea3d52a54b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>booked a trip for 4 ppl, want to add a 5th now</td>\n",
       "      <td>Booking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello team, please confirm if my hotel reserva...</td>\n",
       "      <td>Booking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can’t see the payment options, dropdown just...</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I heard airlines sometimes overbook, what’s yo...</td>\n",
       "      <td>Policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wanna reschedule my train ride to next week</td>\n",
       "      <td>Booking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query      label\n",
       "0     booked a trip for 4 ppl, want to add a 5th now    Booking\n",
       "1  hello team, please confirm if my hotel reserva...    Booking\n",
       "2  i can’t see the payment options, dropdown just...  Technical\n",
       "3  I heard airlines sometimes overbook, what’s yo...     Policy\n",
       "4        wanna reschedule my train ride to next week    Booking"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/bookings.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde191f7-2f78-4883-8666-9c63315f975a",
   "metadata": {},
   "source": [
    "### Define Data Structure for Evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e80d62-fc0b-4582-a64c-12c6eacfd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DataDefinition(\n",
    "    text_columns=[\"query\"],\n",
    "    categorical_columns=[\"label\"],\n",
    "    llm=LLMClassification(input=\"query\", target=\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccb4891-d8c9-4c13-9503-c1a259e1d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data, data_definition=dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83de632-6a5c-4e7b-8f6a-92dfba189469",
   "metadata": {},
   "source": [
    "### Define a Multiclass Prompt and LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6413de84-f0ef-4793-9cc0-eb8c5eaf7111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API key (input is hidden):  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key (input is hidden): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f584a4dc-040d-40df-841d-5d5adca1a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"Classify inqueries by categories\"\n",
    "\n",
    "t = MulticlassClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are classifying user queries.\")],\n",
    "    criteria=base_prompt,\n",
    "    category_criteria={\n",
    "        \"Booking\": \"bookings\",\n",
    "        \"Technical\": \"technical questions\",\n",
    "        \"Policy\": \"questions about policies\",\n",
    "        \"Payment\": \"payment questions\",\n",
    "        \"Escalation\": \"escalation requests\"\n",
    "    },\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\n",
    "    alias=\"bookings\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    column_name=\"query\",\n",
    "    template=t\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c96f1-72e3-4b8a-9b46-8c33d5129300",
   "metadata": {},
   "source": [
    "### Run the Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "186551b6-9825-4849-8cb2-095862c3669b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed prompt 'Classify inqueries by categories...', got preds(200) preds_reasoning(200)\n",
      "Prompt scored: AccuracyScorer: 0.915\n",
      "Prompt 'Classify inqueries by categories...' optimized to 'Classify inquiries into the following ca...'\n",
      "Executed prompt 'Classify inquiries into the following ca...', got preds(200) preds_reasoning(200)\n",
      "Prompt scored: AccuracyScorer: 0.94\n",
      "Prompt 'Classify inquiries into the following ca...' optimized to 'Classify inquiries into the following ca...'\n",
      "Executed prompt 'Classify inquiries into the following ca...', got preds(200) preds_reasoning(200)\n",
      "Prompt scored: AccuracyScorer: 0.93\n"
     ]
    }
   ],
   "source": [
    "optimizer = PromptOptimizer(\"bookings_example\", strategy=\"feedback\")\n",
    "optimizer.set_input_dataset(dataset)\n",
    "await optimizer.arun(judge, \"accuracy\")\n",
    "# sync version\n",
    "# optimizer.run(judge, \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf15d94-31fe-4a8d-91ae-7f25425856aa",
   "metadata": {},
   "source": [
    "### View the Best Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16a85f0-4f13-4348-944c-8432667b7d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify inqueries by categories\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f3946-9547-4716-9b84-536ef99bac47",
   "metadata": {},
   "source": [
    "## Example 3: Tweet Generation Example\n",
    "This tutorial shows how to optimize prompts for generating engaging tweets using Evidently's `PromptOptimizer` API. \n",
    "We'll iteratively improve a tweet generation prompt to maximize how engaging LLM-generated tweets are, according to a classifier.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to define a tweet generation function with OpenAI\n",
    "- How to set up an LLM judge to classify tweet engagement\n",
    "- How to optimize a tweet generation prompt based on feedback\n",
    "- How to inspect the best optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8ead3-2020-4538-a28e-69a8ebb49bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed\n",
    "# !pip install evidently openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6dc460f-73bc-408d-9207-c75d25e76773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.llm.optimization import PromptOptimizer, PromptExecutionLog, Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bac22-e206-49ff-956e-85bfdb9b6dd3",
   "metadata": {},
   "source": [
    "### Define a Tweet Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1defacfb-fcb1-46ad-be92-a328a2006336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API key (input is hidden):  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key (input is hidden): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1324a5cb-fcfc-4936-8189-a9a69eba26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tweet_generation(topic, model=\"gpt-3.5-turbo\", instructions=\"\"):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": f\"Write a short paragraph about {topic}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af8d7d-1101-4193-ae01-6ac373cf37a7",
   "metadata": {},
   "source": [
    "### Define a Tweet Quality Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76adad10-7b03-4a28-9f95-2fdc112f9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of tweets\")],\n",
    "    criteria=\"\"\"\n",
    "Text is ENGAGING if it meets at least one of the following:\n",
    "  • Strong hook (question, surprise, bold statement)\n",
    "  • Uses emotion, humor, or opinion\n",
    "  • Encourages interaction\n",
    "  • Shows personality or distinct tone\n",
    "  • Includes vivid language or emojis\n",
    "  • Sparks curiosity or insight\n",
    "\n",
    "Text is NEUTRAL if it lacks these qualities.\n",
    "\"\"\",\n",
    "    target_category=\"ENGAGING\",\n",
    "    non_target_category=\"NEUTRAL\",\n",
    "    uncertainty=\"non_target\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\"basic_tweet_generation.result\", template=tweet_quality,\n",
    "                provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Tweet quality\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3cb26-7ed8-4e49-a627-bab987e32e6a",
   "metadata": {},
   "source": [
    "### Define a Prompt Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a6c783-f823-4a33-91d0-940a98cc050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(generation_prompt: str, context) -> PromptExecutionLog:\n",
    "    \"\"\"generate engaging tweets\"\"\"\n",
    "    my_topics = [\n",
    "        \"testing in AI engineering is as important as in development\",\n",
    "        \"CI/CD is applicable in AI\",\n",
    "        \"Collaboration of subject matter experts and AI engineers improves product\",\n",
    "        \"Start LLM apps development from test cases generation\",\n",
    "        \"evidently is a great tool for LLM testing\"\n",
    "    ]\n",
    "    tweets = [basic_tweet_generation(topic, model=\"gpt-3.5-turbo\", instructions=generation_prompt) for topic in my_topics * 3]\n",
    "    return PromptExecutionLog(generation_prompt, prediction=pd.Series(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fa391-8c25-4dbf-a83f-2176dabc35eb",
   "metadata": {},
   "source": [
    "### Run the Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e075281-5175-48ba-b304-b4597e58c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed prompt 'You are tweet generator...', got preds(15)\n",
      "Prompt scored: BinaryJudgeScorer: 0.06666666666666667\n",
      "Prompt 'You are tweet generator...' optimized to 'You are a creative tweet generator taske...'\n",
      "Executed prompt 'You are a creative tweet generator taske...', got preds(15)\n",
      "Prompt scored: BinaryJudgeScorer: 1.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = PromptOptimizer(\"tweet_gen_example\", strategy=\"feedback\")\n",
    "optimizer.set_param(Params.BasePrompt, \"You are tweet generator\")\n",
    "await optimizer.arun(run_prompt, scorer=judge)\n",
    "# sync version\n",
    "# optimizer.run(run_prompt, scorer=judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e6abd-1cdc-4425-89a4-451e4108e2d5",
   "metadata": {},
   "source": [
    "### View the Best Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9928fc-8c61-4f09-8aba-b10e978f90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a creative tweet generator tasked with crafting engaging, concise, and dynamic tweets that resonate with audiences. Your tweets should include strong hooks, emotional language, humor, prompts for interaction, and the use of vivid imagery or emojis where appropriate. Focus on delivering content that feels personal, relatable, and encourages readers to engage with the topic or share their thoughts. Aim for a tone that is friendly, enthusiastic, and full of personality, turning complex concepts into enjoyable and easily digestible messages.\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd657be-3f3f-46a2-b77e-9e386ed4355c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
