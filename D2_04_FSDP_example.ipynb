{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b1f163-108f-4587-8b0d-0246158ee528",
   "metadata": {},
   "source": [
    "# FSDP example with Phi-3.5 mini instruct and openassistant-guanaco dataset\n",
    "In this example a network is trained on multiple GPUs with the help of FSDP (Fully Sharded Data Parallel). This approach allows to train networks that are too large to fit into the memory of a single GPU.\n",
    "\n",
    "If we want to use multiple GPUs, we need to write the code to a file and submit the job to the SLURM scheduler, because the JupyterHub that we are using today does not have access to any GPU. This example uses two GPUs on one node, but could be extended simply by adjusting the number of GPUs and nodes in the SLURM script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ee84-e29b-4c05-b124-50e735033760",
   "metadata": {},
   "source": [
    "#### First, we write the python code to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565c4533-5104-4a7c-a688-8b6acb72e17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting phi3_guanaco_fsdp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi3_guanaco_fsdp.py\n",
    "# Import libraries\n",
    "import torch\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pynvml\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    memory_used = []\n",
    "    for device_index in range(device_count):\n",
    "        device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n",
    "        device_info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "        memory_used.append(device_info.used/1024**3)\n",
    "    print('Memory occupied on GPUs: ' + ' + '.join([f'{mem:.1f}' for mem in memory_used]) + ' GB.')\n",
    "\n",
    "\n",
    "# Choose a model and load tokenizer and model (using 4bit quantization):\n",
    "model_name = '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/microsoft--phi-3.5-mini-instruct'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# For multi-GPU training, find out how many GPUs there are and which one we should use:\n",
    "ps = PartialState()\n",
    "num_processes = ps.num_processes\n",
    "process_index = ps.process_index\n",
    "local_process_index = ps.local_process_index\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,  # Added for FSDP\n",
    "    ),\n",
    "    # device_map={'':local_process_index},  # Removed for FSDP\n",
    "    attn_implementation='eager',  # 'eager', 'sdpa', or \"flash_attention_2\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the guanaco dataset\n",
    "guanaco_train = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='train')\n",
    "guanaco_test = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='test')\n",
    "# guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "# guanaco_test = load_dataset('timdettmers/openassistant-guanaco', split='test')\n",
    "guanaco_train = guanaco_train.map(lambda entry: {\n",
    "    'question1': entry['text'].split('###')[1].removeprefix(' Human: '),\n",
    "    'answer1': entry['text'].split('###')[2].removeprefix(' Assistant: ')\n",
    "})\n",
    "guanaco_test = guanaco_test.map(lambda entry: {\n",
    "    'question1': entry['text'].split('###')[1].removeprefix(' Human: '),\n",
    "    'answer1': entry['text'].split('###')[2].removeprefix(' Assistant: ')\n",
    "})\n",
    "guanaco_train = guanaco_train.map(lambda entry: {'messages': [\n",
    "    {'role': 'user', 'content': entry['question1']},\n",
    "    {'role': 'assistant', 'content': entry['answer1']}\n",
    "]})\n",
    "guanaco_test = guanaco_test.map(lambda entry: {'messages': [\n",
    "    {'role': 'user', 'content': entry['question1']},\n",
    "    {'role': 'assistant', 'content': entry['answer1']}\n",
    "]})\n",
    "\n",
    "model.config.use_cache = False  # KV cache can only speed up inference, but we are doing training.\n",
    "\n",
    "# Add low-rank adapters (LORA) to the model:\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=16,\n",
    "    lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    target_modules='all-linear',\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir='output/phi-3.5-mini-instruct-guanaco-fsdp',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 8 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 12 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    # We don't need the following two lines for FSDP (compared to DDP):\n",
    "    # ddp_find_unused_parameters=False,  # Set to False when using gradient checkpointing to suppress warning message.\n",
    "    # log_level_replica='error',  # Disable warnings in all but the first process.\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    logging_strategy='no',\n",
    "    # logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    # logging_steps=10,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    bf16=True,  # mixed precision training\n",
    "    report_to='none',  # disable wandb\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "def formatting_func(entry):\n",
    "    return tokenizer.apply_chat_template(entry['messages'], tokenize=False)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    eval_dataset=guanaco_test,\n",
    "    processing_class=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "if process_index == 0:  # Only print in first process.\n",
    "    if hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "train_result = trainer.train()\n",
    "if process_index == 0:\n",
    "    print(\"Training result:\")\n",
    "    print(train_result)\n",
    "\n",
    "# Print memory usage once per node:\n",
    "if local_process_index == 0:\n",
    "    print_gpu_utilization()\n",
    "\n",
    "# Save model:\n",
    "# Note: This needs to be executed an all python process to work, not only on the first process.\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e5aa1-7732-481f-a8db-451360ba6d74",
   "metadata": {},
   "source": [
    "#### Next, we write a file with the configuration for FSDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e80b63-c5f8-4166-b0dc-4d48c9b4c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fsdp_config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile fsdp_config.yml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: FSDP\n",
    "downcast_bf16: 'no'\n",
    "fsdp_config:\n",
    "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
    "  fsdp_backward_prefetch: BACKWARD_PRE\n",
    "  fsdp_cpu_ram_efficient_loading: true\n",
    "  fsdp_forward_prefetch: false\n",
    "  fsdp_offload_params: false\n",
    "  fsdp_sharding_strategy: FULL_SHARD\n",
    "  fsdp_state_dict_type: FULL_STATE_DICT\n",
    "  fsdp_sync_module_states: true\n",
    "  fsdp_use_orig_params: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: bf16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: c10d\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1769457-c82f-4954-89a5-7b3b47ed72cc",
   "metadata": {},
   "source": [
    "#### Finally, we write the SLURM script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73291081-25e7-4578-a944-716d4e29d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_phi3_guanaco_fsdp.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_phi3_guanaco_fsdp.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=2  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=240GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=16  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load anaconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate /leonardo/pub/userexternal/mpfister/conda_env_martin24\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=$(shuf -i 20000-30000 -n 1)  # Choose a random port\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Set launcher and launcher arguments:\n",
    "export LAUNCHER=\"accelerate launch \\\n",
    "    --num_machines $SLURM_NNODES \\\n",
    "    --num_processes $((SLURM_NNODES * SLURM_GPUS_ON_NODE)) \\\n",
    "    --num_cpu_threads_per_process 8 \\\n",
    "    --main_process_ip $MASTER_ADDR \\\n",
    "    --main_process_port $MASTER_PORT \\\n",
    "    --machine_rank \\$SLURM_PROCID \\\n",
    "    --config_file \\\"fsdp_config.yml\\\" \\\n",
    "    \"\n",
    "# Set training script that will be executed:\n",
    "export PROGRAM=\"phi3_guanaco_fsdp.py\"\n",
    "\n",
    "# Run:\n",
    "time srun bash -c \"$LAUNCHER $PROGRAM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786f174-8231-4e1e-ae39-bff66ffccddc",
   "metadata": {},
   "source": [
    "#### We can now execute the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8cc6fe-ec18-4856-b99a-e1e2f4f5ca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16601525\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_phi3_guanaco_fsdp.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7886c3e1-da04-49b9-bf9d-806083239ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          16601525 boost_usr run_phi3 mpfister  R       0:10      1 lrdn0026\n",
      "          16599181 boost_usr jupyterl mpfister  R    1:07:09      1 lrdn2012\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f36afe-483c-4f44-aefc-be4a2304ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Tue Jun 10 18:13:55 CEST 2025\n",
      "+ hostname\n",
      "lrdn0026.leonardo.local\n",
      "+ nvidia-smi\n",
      "Tue Jun 10 18:13:55 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB            On | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   43C    P0               62W / 460W|      0MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM-64GB            On | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   42C    P0               61W / 467W|      0MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "++ shuf -i 20000-30000 -n 1\n",
      "+ export MASTER_PORT=24802\n",
      "+ MASTER_PORT=24802\n",
      "++ scontrol show hostnames lrdn0026\n",
      "++ head -n 1\n",
      "+ export MASTER_ADDR=lrdn0026\n",
      "+ MASTER_ADDR=lrdn0026\n",
      "+ export OMP_NUM_THREADS=16\n",
      "+ OMP_NUM_THREADS=16\n",
      "+ export 'LAUNCHER=accelerate launch     --num_machines 1     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn0026     --main_process_port 24802     --machine_rank $SLURM_PROCID     --config_file \"fsdp_config.yml\"     '\n",
      "+ LAUNCHER='accelerate launch     --num_machines 1     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn0026     --main_process_port 24802     --machine_rank $SLURM_PROCID     --config_file \"fsdp_config.yml\"     '\n",
      "+ export PROGRAM=phi3_guanaco_fsdp.py\n",
      "+ PROGRAM=phi3_guanaco_fsdp.py\n",
      "+ srun bash -c 'accelerate launch     --num_machines 1     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn0026     --main_process_port 24802     --machine_rank $SLURM_PROCID     --config_file \"fsdp_config.yml\"      phi3_guanaco_fsdp.py'\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "[rank1]:[W610 18:14:27.964955253 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank0]:[W610 18:14:27.259468098 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-06-10 18:14:27,995] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-10 18:14:27,995] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "trainable params: 25,165,824 || all params: 3,846,245,376 || trainable%: 0.6543\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      " 88%|████████▊ | 88{'train_runtime': 193.1288, 'train_samples_per_second': 8.285, 'train_steps_per_second': 0.518, 'train_loss': 1.182539291381836, 'epoch': 0.16}\n",
      "100%|██████████| 100/100 [03:13<00:00,  1.93s/it]\n",
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.182539291381836, metrics={'train_runtime': 193.1288, 'train_samples_per_second': 8.285, 'train_steps_per_second': 0.518, 'total_flos': 2.709130293595341e+16, 'train_loss': 1.182539291381836, 'epoch': 0.16233766233766234})\n",
      "Memory occupied on GPUs: 13.5 + 16.0 GB.\n",
      "/leonardo/pub/userexternal/mpfister/conda_env_martin24/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/leonardo/pub/userexternal/mpfister/conda_env_martin24/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "\n",
      "real\t3m51.644s\n",
      "user\t0m0.266s\n",
      "sys\t0m0.007s\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-16601525.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61e8d3-5625-4a78-8a08-32ea4158a581",
   "metadata": {},
   "source": [
    "#### Finally, we can clean up and delete the files that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a52fe7c-9bfe-45d0-84ed-9287c9c84f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm fsdp_config.yml phi3_guanaco_fsdp.py run_phi3_guanaco_fsdp.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d8108-1e84-45c2-8560-af8958deb599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
