{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cd3a8e-b73f-4481-95d1-efea9317baa5",
   "metadata": {},
   "source": [
    "# Tokenization and Embeddings\n",
    "\n",
    "Before feeding text into a Transformer model, it must be converted into a numerical format that the model can understand. This process involves two key steps: **Tokenization** and **Embeddings**.\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units called tokens. Tokens can be words, subwords, or characters. Different tokenization methods are used depending on the model architecture and language.\n",
    "\n",
    "### Types of Tokenization:\n",
    "\n",
    "1. **Word Tokenization**: Splits text into individual words (e.g., \"The quick brown fox\").\n",
    "2. **Subword Tokenization**: Splits words into smaller subword units (e.g., \"Transformer\" -> \"Trans\", \"##former\").\n",
    "3. **Character Tokenization**: Splits text into individual characters (e.g., \"hello\" -> \"h\", \"e\", \"l\", \"l\", \"o\").\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are dense vector representations of tokens. They map tokens to continuous vector spaces, capturing semantic meaning and relationships. \n",
    "\n",
    "### Types of Embeddings:\n",
    "\n",
    "1. **Static Embeddings**: Pre-trained on large corpora and remain fixed during training (e.g., Word2Vec, GloVe).\n",
    "2. **Contextual Embeddings**: Generated dynamically by Transformer models based on the context of the surrounding words (e.g., BERT, GPT).\n",
    "\n",
    "Let's explore these concepts in detail and understand how they are applied in practice using the Huggingface `transformers` library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4357e1-1271-4360-8410-4fd21ea509c9",
   "metadata": {},
   "source": [
    "## Tokenization with Huggingface Transformers\n",
    "\n",
    "The `transformers` library provides a wide range of pre-trained tokenizers that are optimized for different Transformer models. These tokenizers convert raw text into token IDs that can be fed into a model.\n",
    "\n",
    "### Example: Tokenizing Text using a Pre-trained Tokenizer\n",
    "Let's use the BERT tokenizer to tokenize some example text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1accb9-8b8a-4ecb-a7b0-2870e95f7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb60fc8-d486-4507-b728-6cb0f637178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0383c84-821c-40f2-86ab-361b34b69157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"Transformers are revolutionizing NLP!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1413d-81cd-41bc-bde3-b6505f72f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"Simeon Harrison is so competent and handsome, he is a unicorn trainer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227e64b-66a8-4735-ab94-62ca28033bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e7d82-8353-486d-92ef-55fc5a2dabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_2 = tokenizer.tokenize(text_2)\n",
    "print(\"Tokens:\", tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c6590-e3c8-42e0-b366-15a8ad14fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"Large language models on supercomputers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69eb054-fe3f-44a0-8c7d-1ec7b88cb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_3 = tokenizer.tokenize(text_3)\n",
    "print(\"Tokens:\", tokens_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b9d33-cabc-43e8-a987-620b3d29d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the actual numeric representations of the tokens\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fb0d5-2ce9-4a86-8b66-678f8b156693",
   "metadata": {},
   "source": [
    "## Exploring Different Tokenization Methods\n",
    "\n",
    "Huggingface provides several tokenization methods to handle various languages and tasks. Let's explore how different tokenization methods handle the same input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904c76d-d4f8-4087-a0df-1e5c64561809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of different tokenizers\n",
    "bert_base_uncased = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\"\n",
    "gpt2 = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/openai-community--gpt2\"\n",
    "xlm_roberta_base = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/facebookai--xlm-roberta-base\"\n",
    "\n",
    "tokenizers = [bert_base_uncased, gpt2, xlm_roberta_base]\n",
    "\n",
    "for tok_name in tokenizers:\n",
    "    print(f\"\\nUsing {tok_name} tokenizer:\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567d830-4c26-4e52-bdef-870a044dacc0",
   "metadata": {},
   "source": [
    "## Tokenizing a Whole Dataset\n",
    "To tokenize a whole dataset, we first need to define a tokenize funktion.\n",
    "We can then use the `map()` method to apply the function to the whole dataset.\n",
    "But before that, we need to load a dataset. Let's use the emotions dataset from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519e77b-e14e-4a42-b473-b5940edc7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563c0be-d841-430c-8b70-b8ea42233606",
   "metadata": {},
   "source": [
    "We would like to know which tokenizer is behind the variable tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1badf-0e29-4988-98f3-7c9f41202593",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfa903-2a95-44d5-9f6b-68e6583048fd",
   "metadata": {},
   "source": [
    "As you can see, we get a lot of information about the bert-base-uncased tokenizer.\n",
    "Now, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4adccf-f8c4-443e-9389-a88019317a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/dair-ai--emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353a2f5-d4fc-4b3a-abb1-db007a52d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfbaac-06e3-441e-8fee-c367317d3000",
   "metadata": {},
   "source": [
    "Here, you can see the splits the dataset comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa9904-bfca-4f50-98ee-9ba3bf9d9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddae7df-46fd-4d3b-9da8-fecbe52fbc99",
   "metadata": {},
   "source": [
    "To see `tokenize()` in action, we pass two examples from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ec08b-7e3d-4390-9727-d4f3018d620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab7778-38d5-4573-b81f-d3fe29affafe",
   "metadata": {},
   "source": [
    "Here you can see, that a tokenized dataset does not just consist of the token ids, but much more, which the model uses for training.  \n",
    "Now, let's apply `map()`. `batch_size=None` means the tokenize function is mapped to the whole dataset as one big batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88cbc42-19b2-4fbe-be57-3247ba686c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1315287-cec7-4c71-b229-a68ad9d69794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67019e2-a7ea-400a-a791-17a249a81a2b",
   "metadata": {},
   "source": [
    "## Embeddings with Transformers\n",
    "\n",
    "Once text is tokenized, it is converted into embeddings before being fed into the Transformer model. Transformers then create **contextual embeddings** that capture the meaning of words in context.\n",
    "\n",
    "### Example: Extracting Embeddings from a Pre-trained Model\n",
    "Let's use a pre-trained BERT model to extract embeddings for some example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a65b6-3e9d-4941-bc13-0a5c6b553d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094165fd-41d1-481a-ad9c-868e94ae438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text and convert to input IDs\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae7764-aee2-4b9c-bafd-7f1a28304a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2841471-4819-42fa-bc27-1f2b0ec243f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings (output of the model's hidden states)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden state\n",
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23789c3c-1264-4dff-9bb3-04813f3f4b36",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "The `with torch.no_grad():` statement creates a context in which operations on tensors will not track gradients.\n",
    "This is used during inference (when you are using the model to make predictions rather than training it) to save memory and speed up computation by not storing information needed to compute gradients.\n",
    "\n",
    "`**inputs:` unpacks the inputs dictionary, passing its contents as keyword arguments to the model. `inputs` usually contains keys like:\n",
    "- 'input_ids': The tokenized input text.\n",
    "- 'attention_mask': A mask to differentiate real tokens from padding tokens.\n",
    "- 'token_type_ids' (optional): Indicates the segments in tasks like question answering.\n",
    "\n",
    "These inputs are then fed into the model for a forward pass.\n",
    "\n",
    "`outputs:` is a variable that will store the output of the model's forward pass. The specific structure of outputs depends on the type of model used. For most transformer models, the output will be a named tuple or an OrderedDict containing several attributes.\n",
    "\n",
    "`outputs.last_hidden_state:` extracts the \"last hidden state\" from the model's output. The last hidden state is a tensor representing the final layer's hidden states (embeddings) for each token in the input sequence.  \n",
    "The `shape` of last_hidden_state is usually `(batch_size, sequence_length, hidden_size)`, where:\n",
    "- batch_size: The number of input sequences in the batch.\n",
    "- sequence_length: The number of tokens in each input sequence.\n",
    "- hidden_size: The size of the hidden states (e.g., 768 for BERT base).  \n",
    "Purpose: The last hidden state is a contextual representation of each token in the input sequence, capturing the meaning of the token in the context of the entire sequence.\n",
    "\n",
    "Let's print that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fb0f6-2525-46a8-b1b0-9c5297e7bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Hidden State Shape:\", last_hidden_state.shape)\n",
    "print(\"Embeddings for each token:\\n\", last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e8024-1b83-4de3-af60-164e572f5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef926d3-1c0b-47e9-addf-0964e1e37dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text\n",
    "text = \"The fat ginger cat sat on a mat\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b343e4-ea8b-4892-9068-60880f548029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to numpy array\n",
    "embeddings = last_hidden_state[0].numpy()\n",
    "\n",
    "# Get the special tokens' IDs\n",
    "special_tokens_ids = tokenizer.all_special_ids\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Filter out special tokens and their embeddings\n",
    "filtered_embeddings = []\n",
    "filtered_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64dac10-71d6-4f7a-ba4e-a99082592ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token_id in enumerate(inputs['input_ids'][0]):\n",
    "    if token_id not in special_tokens_ids:\n",
    "        filtered_embeddings.append(embeddings[i])\n",
    "        filtered_tokens.append(tokens[i])\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(filtered_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80039479-869d-409f-8682-b7072583cd86",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "The `.numpy()` method converts the PyTorch tensor to a NumPy array.\n",
    "This step is necessary because PCA in scikit-learn operates on NumPy arrays, not PyTorch tensors.\n",
    "Note: For this conversion to work, the tensor must be on the CPU. If the tensor is on a GPU, you should move it to the CPU first using .cpu().  \n",
    "E.g.: `embeddings = last_hidden_state[0].cpu().numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39405aca-5dab-40b2-9156-b2631a18c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c='blue')\n",
    "\n",
    "# Annotate the points with tokens\n",
    "for i, token in enumerate(filtered_tokens):\n",
    "    plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], token, fontsize=9)\n",
    "\n",
    "plt.title(\"2D PCA of Token Embeddings\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f73a2-cdb8-4f2c-aa64-4e35806c9d4a",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Working with Tokenization and Embeddings\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Choose a different pre-trained model from the Huggingface Hub.\n",
    "2. Tokenize a new text input.\n",
    "3. Extract and visualize embeddings for the tokens.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Choose a model (e.g., \"distilbert-base-uncased\").\n",
    "2. Tokenize the following text: \"Huggingface makes NLP easy and accessible.\"\n",
    "3. Extract embeddings using the last hidden state of the model.\n",
    "\n",
    "Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff5979-4d7c-4222-9fd2-f4ccf8a29d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f89739-7b86-4d62-ada4-a56a18d7d436",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (Possible) Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0b1c4-8f0d-4ce0-9042-44bcc8feb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose a model\n",
    "model_dir = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/distilbert--distilbert-base-uncased\"  # Change this to any model of your choice\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModel.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8e5a1-c59a-4eef-bba0-07e32ade7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize the new text\n",
    "new_text = \"Huggingface makes NLP easy and accessible.\"\n",
    "inputs = tokenizer(new_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa25a6b-077f-415f-903b-955dfea850e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden state\n",
    "new_last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef312c3-199d-4480-8146-c1d5113b97a4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the concepts of tokenization and embeddings, which are crucial for preparing text input for Transformer models. We demonstrated how to tokenize text using different methods and extract contextual embeddings from pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df86d3-451a-48b8-913d-4ae00ccaef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
