{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68bcd6-bca8-4376-b231-0c8c84c532e4",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96197c4-9bdf-4895-a209-f97a30660b66",
   "metadata": {},
   "source": [
    "[Gradio](https://www.gradio.app) can enable simple web interfaces to your software. In this example, we are using Gradio to get a simple chat interface to a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e10fab-fc0b-41c0-a794-8f73ae823ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gradio_example.py\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import gradio as gr\n",
    "import os\n",
    "import random\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "hostname = socket.gethostname().split('.')[0]\n",
    "port = random.randint(10000, 50000)\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# ! Change the name trainee user name to the name in your personal URL: !\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "trainee_user = 'trainee01'\n",
    "\n",
    "print('Open the following URL in your webbrowser:')\n",
    "print(f'https://hpctraining.org/{trainee_user}/proxy/absolute/{hostname}:{port}/')\n",
    "print('')\n",
    "sys.stdout.flush()\n",
    "\n",
    "model_name = '/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/microsoft--phi-3.5-mini-instruct'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='cuda',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    ")\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "def get_answer(question, history=[]):\n",
    "    history.append(\n",
    "        {'role': 'user', 'content': question}\n",
    "    )\n",
    "    result = pipe(history, max_new_tokens=500, return_full_text=False)\n",
    "    return result[0]['generated_text'].strip()\n",
    "    # return question\n",
    "\n",
    "chat_interface = gr.ChatInterface(get_answer, type='messages')\n",
    "chat_interface.launch(share=False, server_name=ip, server_port=port, root_path=f'/{trainee_user}/proxy/absolute/{hostname}:{port}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0e4a77-b509-4e9a-8b44-d735c8539ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_gradio_example.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_gradio_example.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:10:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load anaconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate /leonardo/pub/userexternal/mpfister/conda_env_martin24\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Run AI scripts:\n",
    "python3 gradio_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f8e6-6855-491d-86dd-46de21d4d753",
   "metadata": {},
   "source": [
    "Now submit the SLURM job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaf7a89-88ee-45f7-9fd0-fb5820db25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 12959420\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_gradio_example.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ddfe-44de-4b68-aab8-00f08a610e8c",
   "metadata": {},
   "source": [
    "Execute `squeue` to see, if your job is already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e10a89e-425e-4570-993b-c595869183e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          12959420 boost_usr run_grad mpfister PD       0:00      1 (None)\n",
      "          12952669 boost_usr jupyterl mpfister  R    5:29:26      1 lrdn3456\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072291e5-fea9-4821-9958-2da0b0ada7b5",
   "metadata": {},
   "source": [
    "Once your job is running, look at the output of the job using the following command (replace the number with the JOBID from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d325f17-2e5c-4b26-82d4-98e5b4f52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Mon Feb 24 21:39:29 CET 2025\n",
      "+ hostname\n",
      "lrdn0151.leonardo.local\n",
      "+ nvidia-smi\n",
      "Mon Feb 24 21:39:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB            On | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   43C    P0               63W / 477W|      0MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ python3 gradio_example.py\n",
      "Open the following URL in your webbrowser:\n",
      "https://hpctraining.org/trainee01/proxy/absolute/lrdn0151:17228/\n",
      "\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-12959420.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aaff63-60eb-4cda-bbac-fb214b0a588a",
   "metadata": {},
   "source": [
    "Finally, when you are finished, please cancel the SLURM job to free the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bd34e4-fa5a-426d-b51b-218269740ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 12959420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c020f6-e785-41f6-bb49-58d56994389c",
   "metadata": {},
   "source": [
    "If you want to, you can also delete the files that we create above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cc3e8b-6303-46d4-9051-f3551e527e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm gradio_example.py run_gradio_example.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049395-b496-4412-abb7-1c5a7f592cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
