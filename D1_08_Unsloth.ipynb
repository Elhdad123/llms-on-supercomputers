{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c272574-6a76-40c2-b7fd-9cadce7ef1c5",
   "metadata": {},
   "source": [
    "## Unsloth: Optimizing Training and Inference Performance\n",
    "\n",
    "For many software algorithms, the performance does not only depend on the number and kind of calculations performed. Instead, the exact order and the size of chunks has an enormous influence on the calculation speed.\n",
    "For large language models, a library called `unsloth` contains optimized GPU kernels created by manually deriving all compute heavy math steps. By using these optimized kernels, a significant speed-up can be obtained.\n",
    "\n",
    "### Key Techniques in Unsloth:\n",
    "\n",
    "1. **Efficient Data Loading**: Optimizing data pipelines to reduce latency and improve throughput during training.\n",
    "2. **Batching and Padding Strategies**: Dynamically adjusting batch sizes and minimizing padding to optimize memory usage.\n",
    "3. **Half-Precision and Quantized Inference**: Using mixed precision or quantized models to speed up inference and reduce memory footprint.\n",
    "4. **Model Pruning and Distillation**: Reducing the size of the model by removing redundant parameters or training smaller models to mimic larger ones.\n",
    "\n",
    "### Benefits of Unsloth:\n",
    "\n",
    "- **Reduced Training Time**: Optimizing data loading and model architecture reduces the time required for each epoch.\n",
    "- **Lower Memory Usage**: Using techniques like mixed precision and quantization reduces the amount of GPU memory required.\n",
    "- **Faster Inference**: Optimizing the model for deployment can significantly reduce latency during inference.\n",
    "\n",
    "### Hands-On Example: Efficient Data Loading and Mixed Precision Training\n",
    "\n",
    "In this example, we take the example from the previous notebook (\"PEFT\") and adjust them to use `unsloth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ffe0c8-2fc5-4885-8ea1-881beb6b2447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/conda_container_env/lib/python3.11/site-packages/plink/gui.py:33: UserWarning: Plink failed to import tkinter, GUI will not be available\n",
      "  warnings.warn('Plink failed to import tkinter, GUI will not be available')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "[2025-01-22 11:43:09,593] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "## Instead of:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "## use:\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8c4214-75f3-4936-b9be-263dab081a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM-64GB. Max memory: 63.423 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Choose a model and load tokenizer and model (using 4bit quantization):\n",
    "model_name = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/unsloth--Phi-3.5-mini-instruct-bnb-4bit\"\n",
    "# model_name = \"unsloth/Phi-3.5-mini-instruct-bnb-4bit\"\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "## Instead of:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(...)\n",
    "## use: \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    ## Instead of:\n",
    "    # quantization_config=BitsAndBytesConfig(...)\n",
    "    ## use:\n",
    "    load_in_4bit=True,\n",
    "    # device_map='cuda:0',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side = 'right'\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7843e6-8ca3-4309-92a6-358973336018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load the guanaco dataset\n",
    "guanaco_train = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='train')\n",
    "# guanaco_test = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='test')\n",
    "# guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "# guanaco_test = load_dataset('timdettmers/openassistant-guanaco', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814d0851-8920-465d-9fb3-2c776fcef64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "guanaco_train = guanaco_train.map(lambda entry: {\n",
    "    'question1': entry['text'].split('###')[1].removeprefix(' Human: '),\n",
    "    'answer1': entry['text'].split('###')[2].removeprefix(' Assistant: ')\n",
    "})\n",
    "guanaco_train = guanaco_train.map(lambda entry: {'messages': [\n",
    "    {'role': 'user', 'content': entry['question1']},\n",
    "    {'role': 'assistant', 'content': entry['answer1']}\n",
    "]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65644a2c-2be2-40a8-be2f-ea57e3ef1aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.1.5 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "## Instead of:\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type='CAUSAL_LM',\n",
    "#     r=16,\n",
    "#     lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "#     bias='none',\n",
    "#     target_modules='all-linear',\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n",
    "## use:\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,  # rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0.05,  # Unsloth supports any, but = 0 is optimized\n",
    "    bias='none',  # Unsloth supports any, but = 'none' is optimized\n",
    "    # Unsloth does not allow 'all-linear' => manually specify target modules: \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    use_gradient_checkpointing='unsloth',  # True or 'unsloth' for very long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0353a79e-5eb9-436a-b789-a3c83d1a8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir='output/unsloth-phi-3.5-mini-instruct-guanaco',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 8 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 12 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    bf16=True,  # mixed precision training\n",
    "    report_to='none',  # disable wandb\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cff35a-37d2-41c2-ae86-b4ec5e164dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(entry):\n",
    "    return tokenizer.apply_chat_template(entry['messages'], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719e9d65-236e-4c2c-a58c-401ddf5fbdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166a8179e3844c3db864adfcd7532b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    processing_class=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab99044-63b2-4679-91bb-87ca62e0b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 9,846 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 8 | Total steps = 100\n",
      " \"-____-\"     Number of trainable parameters = 29,884,416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.132800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.1995847034454346, metrics={'train_runtime': 118.8432, 'train_samples_per_second': 6.732, 'train_steps_per_second': 0.841, 'total_flos': 1.230171564294144e+16, 'train_loss': 1.1995847034454346, 'epoch': 0.08123476848090982})\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "print(\"Training result:\")\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475fa01-1660-40ea-99f8-fdac0ecaea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f261c-eff9-4335-bc9f-8fc4c2dd3c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909731ac-8057-4ff6-a0a5-46bfb9731797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f34e4a-2f3f-44d4-b3b4-91b53cd6253f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
