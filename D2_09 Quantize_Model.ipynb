{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68bcd6-bca8-4376-b231-0c8c84c532e4",
   "metadata": {},
   "source": [
    "## Quantize a model and save it in quantized state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96197c4-9bdf-4895-a209-f97a30660b66",
   "metadata": {},
   "source": [
    "This notebook shows how you can load the unquantized merged model with bitsandbytes quantization and save it as quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e10fab-fc0b-41c0-a794-8f73ae823ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting quantize_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile quantize_model.py\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_path = 'output/phi-3.5-mini-instruct-guanaco-fsdp_merged'\n",
    "quant_path = model_path + '_bnb'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save_pretrained(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "print(f'Quantized model saved to \"{quant_path}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0e4a77-b509-4e9a-8b44-d735c8539ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_quantize_model.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_quantize_model.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:10:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load anaconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate /leonardo/pub/userexternal/mpfister/conda_env_martin29\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Run AI scripts:\n",
    "python3 quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f8e6-6855-491d-86dd-46de21d4d753",
   "metadata": {},
   "source": [
    "Now submit the SLURM job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaf7a89-88ee-45f7-9fd0-fb5820db25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16602244\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_quantize_model.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ddfe-44de-4b68-aab8-00f08a610e8c",
   "metadata": {},
   "source": [
    "Execute `squeue` to see, if your job is already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e10a89e-425e-4570-993b-c595869183e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          16602244 boost_usr run_quan mpfister CF       0:02      1 lrdn2589\n",
      "          16599181 boost_usr jupyterl mpfister  R    1:38:29      1 lrdn2012\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072291e5-fea9-4821-9958-2da0b0ada7b5",
   "metadata": {},
   "source": [
    "Once your job is running, look at the output of the job using the following command (replace the number with the JOBID from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d325f17-2e5c-4b26-82d4-98e5b4f52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ date\n",
      "Tue Jun 10 18:45:26 CEST 2025\n",
      "+ hostname\n",
      "lrdn2589.leonardo.local\n",
      "+ nvidia-smi\n",
      "Tue Jun 10 18:45:26 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM-64GB            On | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   43C    P0               65W / 475W|      0MiB / 65536MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+ python3 quantize_model.py\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]\n",
      "[2025-06-10 18:45:55,569] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Quantized model saved to \"output/phi-3.5-mini-instruct-guanaco-fsdp_merged_bnb\".\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-16602244.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c020f6-e785-41f6-bb49-58d56994389c",
   "metadata": {},
   "source": [
    "If you want to, you can also delete the files that we create above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cc3e8b-6303-46d4-9051-f3551e527e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm quantize_model.py run_quantize_model.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049395-b496-4412-abb7-1c5a7f592cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
