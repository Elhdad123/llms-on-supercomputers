{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78131162-a068-41cb-b1e5-4f80b03cdfa1",
   "metadata": {},
   "source": [
    "# Prompt Engineering Essentials\n",
    "\n",
    "The D3 notebooks will cover the essential topics of prompt engineering, beginning with inference in general and an introduction to LangChain. We will then cover the topics of prompt templates and parsing and will then go on to the concept of creating chains and connecting these in different ways to build more sophisticated constructs to make the most of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d12851-f8e8-4143-8ec0-da82284066a0",
   "metadata": {},
   "source": [
    "## API vs. Locally Hosted LLM\n",
    "Using the an API-hosted LLM (e.g. OpenAI) is like renting a powerful car ‚Äî it‚Äôs ready to go, but you mustn't tinker with the inner workings of the engine and you pay each time you drive.\n",
    "Using a locally hosted model is like buying your own vehicle ‚Äî more upfront work and maintenance, but full control, privacy, and no cost per use, apart from footing the energy bill.\n",
    "\n",
    "| **Aspect**                 | **API-based (e.g. OpenAI)**                          | **Local Model (e.g. Mistral, PyTorch + LangChain)**        |\n",
    "|---------------------------|------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Setup time**            | Minimal ‚Äì just an API key                            | Requires downloading and managing the model                 |\n",
    "| **Hardware requirement**  | None (runs in the cloud)                             | Requires a GPU (sometimes large memory)                     |\n",
    "| **Latency**               | Network-dependent                                    | Faster inference (once model is loaded)                     |\n",
    "| **Privacy / Data control**| Data sent to external servers                      | Data stays on your infrastructure                         |\n",
    "| **Cost**                  | Pay-per-use (based on tokens)                        | Free at inference (after download), but uses your compute   |\n",
    "| **Scalability**           | Handled by provider                                  | You manage and scale infrastructure                         |\n",
    "| **Flexibility**           | Limited to provider's models and settings            | Full control: quantization, fine-tuning, prompt handling    |\n",
    "| **Offline use**           | Not possible                                       | Yes, after initial download                               |\n",
    "| **Customizability**       | No access to internals                             | You can modify and extend anything                        |\n",
    "\n",
    "**Using an API (e.g. OpenAI)** <br>\n",
    " - You use OpenAI or ChatOpenAI class from LangChain\n",
    " - LangChain sends your prompt to api.openai.com\n",
    " - You don‚Äôt manage the model, only the request and response\n",
    "\n",
    "```\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(api_key=\"...\", model=\"gpt-4\")\n",
    "response = llm.invoke(\"Summarize this legal clause...\")\n",
    "```\n",
    "üìù You can store your API key in different ways, it is common, however, to set it as an **environment variable**.\n",
    "Note, that LangChain automatically looks up any environment variable with the name **`OPENAI_API_KEY`** automatically when making a connection to OpenAI. \n",
    "```\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'my_API_key_123'\n",
    "llm = ChatOpenAI(api_key=os.environ['OPENAI_API_KEY'], model=\"gpt-4\")\n",
    "```\n",
    "Alternatively, you could just pass in the openai key via a string (not very secure, you should NEVER hard-code your API keys), or even just save it somewhere on your computer in a text file and then read it in:\n",
    "```\n",
    "f = open('C:\\\\Users\\\\Simeon\\\\Desktop\\\\openai.txt')\n",
    "api_key = f.read()\n",
    "llm = OpenAI(openai_api_key=api_key)\n",
    "```\n",
    "**Using a Local Model (e.g. Mistral, LLaMA)**<br>\n",
    " - You load the model and tokenizer using Hugging Face Transformers\n",
    " - You wrap the pipeline using HuggingFacePipeline or similar in LangChain\n",
    " - You manage memory, GPU allocation, quantization, etc.\n",
    "```\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = ChatHuggingFace(llm=HuggingFacePipeline(pipeline=pipe))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97785f-47e8-4e33-98a1-366843cdd23d",
   "metadata": {},
   "source": [
    "## Basic Setup for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5365e87-dbae-4f26-871f-74f672fc12b9",
   "metadata": {},
   "source": [
    "Apart from the usual suspects of Pytorch and Huggingface libraries, we get our first imports of the LangChain library and some of its classes.\n",
    "\n",
    "Since we want to show you how to how to work with LLMs that are not part of the closed OpenAI and Anthropic world, we are going to show you how to work with open and downloadable models. As it makes no sense for all of us to download the models and store them in our home directory, we've done that for your before the start of the course. You can find the path to the models down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fbb51f-032e-4b72-83d5-37da49f8dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43e856-81b6-4509-b0ac-227a096d2e38",
   "metadata": {},
   "source": [
    "If you choose to work with a model such as `meta-llama/Llama-3.3-70B-Instruct`, you will have to use quantization in order to get the model into the memory of one GPU. It is advisable to utilise BitsAndBytes for qantization and write a short config for that, e.g.:\n",
    "```\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for efficiency\n",
    ")\n",
    "```\n",
    "However, beware, a model of that size takes roughly 30 minutes to load...\n",
    "In this course we do not want to wait around for that long, so we will use a smaller model called [Nous-Hermes-2-Mistral-7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71156aa-4c3c-420e-8345-f5052c0655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = \"/gpfs/data/fs70824/LLMs_models_datasets/models\" # on VSC5\n",
    "path_to_model = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/Nous-Hermes-2-Mistral-7B-DPO\" # on Leonardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b24036b-39a6-4bea-b60f-04e2eb27eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you have an API key for a model hosted in the cloud:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key (input is hidden): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d0ca09-2bce-4761-b6b0-6503f5fb0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#model_name = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "#cache_dir = path_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79281d6e-2dc6-4651-94f6-53b56d7152f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc49b2bf3304f19989154246cfc878e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.55.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    #model_name,\n",
    "    path_to_model,\n",
    "    #cache_dir=cache_dir,\n",
    "    device_map=\"auto\",\n",
    "    #quantization_config=quantization_config, # This is what you would need for the LLama3-70B (and similar) models\n",
    "    local_files_only=True,  # Prevent any re-downloads\n",
    "    # trust_remote_code=True # Should model need to be downloaded from Hugging Face\n",
    ")\n",
    "\n",
    "# Verify model config\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719deb1f-d1a8-42db-8e91-22ec586f6b15",
   "metadata": {},
   "source": [
    "Now, let's try out a prompt or two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f803804-9451-43b6-9b6e-427470a07b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Can you give me some facts about it?\n",
      "\n",
      "The capital of France is Paris. Paris is the largest city in France and is located in the northern part of the country. It is situated on the Seine River and is known for its beautiful architecture, art, and culture.\n",
      "\n",
      "Here are some interesting facts about Paris:\n",
      "\n",
      "1. Paris is home to some of the world‚Äôs most famous landmarks, including the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral.\n",
      "\n",
      "2. The city is often referred to as the ‚ÄúCity of Light‚Äù due to its role in the Age of Enlightenment and its status as a major center of education and ideas.\n",
      "\n",
      "3. Paris is known for its fashion industry and is home to some of the world‚Äôs most famous designers and fashion houses.\n",
      "\n",
      "4. The city is also famous for its cuisine, with dishes such as croissants, escargot, and macarons originating in France.\n",
      "\n",
      "5. Paris is a major transportation hub, with an extensive network of buses, trains, and subways that connect the city to the rest of France and Europe.\n",
      "\n",
      "6. The city is divided into 20\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France? Can you give me some facts about it?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=250)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1bb25-f65d-48ba-9517-00be6a1afa3d",
   "metadata": {},
   "source": [
    "**Not bad, however, we can do better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6cf67-cc6e-44fe-8fe4-71c4f646fbae",
   "metadata": {},
   "source": [
    "Let's try out an OpenAI model. You need your own OpenAI API to be able to execute the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd6df89-f617-4f3b-94b1-0f015d6c17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API key (input is hidden):  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key (input is hidden): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea91b5df-e824-43d2-a609-d1b822a36efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. Here are some interesting facts about the city:\n",
      "\n",
      "1. **Historical Significance**: Paris has a rich history dating back over 2,000 years. It was originally a settlement by a Celtic tribe called the Parisii.\n",
      "\n",
      "2. **Cultural Hub**: The city is renowned for its museums, including the Louvre, which is the world's largest and most visited art museum.\n",
      "\n",
      "3. **Iconic Landmarks**: Paris is home to many famous landmarks, such as the Eiffel Tower, Notre-Dame Cathedral, and the Arc de Triomphe.\n",
      "\n",
      "4. **Fashion Capital**: Known as the fashion capital of the world, Paris hosts major fashion events like Paris Fashion Week and is home to numerous haute couture houses.\n",
      "\n",
      "5. **Gastronomy**: The city boasts a rich culinary scene, with countless bistros, cafes, and Michelin-starred restaurants. French cuisine is celebrated globally.\n",
      "\n",
      "6. **Language**: The official language is French, and the city is known for its efforts to promote and preserve the French language.\n",
      "\n",
      "7. **Education and Research**: Paris is home to prestigious educational institutions, including Sorbonne University and the √âcole Normale Sup√©rieure.\n",
      "\n",
      "8. **Art and Literature\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Simple one-shot prompt, no roles\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"What is the capital of France? Can you give me some facts about it?\",\n",
    "    max_output_tokens=250\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7cbcb-f127-474d-9db4-b0d0daa705ea",
   "metadata": {},
   "source": [
    "## Enter LangChain\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a powerful open-source framework designed to help developers build applications using LLMs. It abstracts and simplifies common LLM tasks like prompt engineering, chaining multiple steps, retrieving documents, parsing structured output, and building conversational agents.\n",
    "\n",
    "LangChain supports a wide range of models (OpenAI, Hugging Face, Cohere, Anthropic, etc.) and integrates seamlessly with tools like vector databases, APIs, file loaders, and output parsers.\n",
    "\n",
    "---\n",
    "### LangChain Building Blocks\n",
    "\n",
    "```\n",
    "+-------------------+\n",
    "|   PromptTemplate  |  ‚Üê Create structured prompts\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "|       LLM         |  ‚Üê Connect to local or remote LLM\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "| Output Parsers    |  ‚Üê Extract structured results (e.g. JSON)\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "| Chains / Agents   |  ‚Üê Combine steps into flows\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "| Memory / Tools    |  ‚Üê Use search, APIs, databases, etc.\n",
    "+-------------------+\n",
    "```\n",
    "---\n",
    "\n",
    "### Core LLM/ChatModel Methods in LangChain\n",
    "How to do inference with LangChain:\n",
    "\n",
    "| **Method**       | **Purpose**                                               | **Input Type**         | **Output Type**         |\n",
    "|------------------|------------------------------------------------------------|-------------------------|--------------------------|\n",
    "| `invoke()`        | Handles a **single input**, returns one response           | `str` or `Message(s)`   | `str` / `AIMessage`      |\n",
    "| `generate()`      | Handles a **batch of inputs**, returns multiple outputs     | `list[str]`             | `LLMResult`              |\n",
    "| `batch()`         | Batched input, returns a flat list of outputs              | `list[str]`             | `list[str]` / Messages   |\n",
    "| `stream()`        | Streams the output as tokens are generated                 | `str` / `Message(s)`    | Generator (streamed text)|\n",
    "| `ainvoke()`       | Async version of `invoke()`                                | `str` / `Message(s)`    | Awaitable result         |\n",
    "| `agenerate()`     | Async version of `generate()`                              | `list[str]`             | Awaitable result         |\n",
    "\n",
    "Before we use one of these methods, we need to create a pipeline and apply the LangChain wrapper to the pipeline, so we create a format that LangChain can call with .invoke() or .generate() etc. If we use an remotly hosted LLM, which we access through an API, we do not need the pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03764fd1-ff93-4484-8cb6-be91bde9dd9d",
   "metadata": {},
   "source": [
    "This is how you could use an cloud-hosted API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a32c1174-16f2-4825-89d4-d8833959adc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is home to the largest volcano in the solar system, Olympus Mons. This massive shield volcano stands about 13.6 miles (22 kilometers) high, which is nearly three times the height of Mount Everest! Its base is roughly the size of the state of Arizona, making it an impressive feature on the Martian landscape.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create an LLM that talks to OpenAI (reads OPENAI_API_KEY from env)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,     # like HF's temperature\n",
    "    max_tokens=150       # analogous to HF's max_new_tokens\n",
    ")\n",
    "\n",
    "# Use it just like your HuggingFacePipeline example:\n",
    "print(llm.invoke(\"Here is a fun fact about Mars:\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4a112-eb0b-4273-98e4-a827d918534a",
   "metadata": {},
   "source": [
    "For a locally hosted model use the Hugging Face text_pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "338c010a-d31f-4a4e-9118-83a66673d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd95c6-e389-47b1-83c1-c26c05b18061",
   "metadata": {},
   "source": [
    "#### llm.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2f8136-d8dc-4095-85b7-d9da665f05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a fun fact about Mars: it has the largest volcano in the entire solar system, called Olympus Mons. Olympus Mons is so big it could easily swallow Mount Everest whole and still have room left over for the Grand Canyon.\n",
      "\n",
      "To give you an idea of just how vast Olympus Mons is, let‚Äôs compare it with Earth‚Äôs own Mount Everest, which is the highest mountain on our planet. Mount Everest is 29,029 feet (8,848 meters) tall from its base to its peak. By comparison, Olympus Mons is 13.6 miles (22 kilometers) tall and 374 miles (602 kilometers) in diameter.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke('Here is a fun fact about Mars:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96043523-d4ac-474a-abf8-cabec94c35ed",
   "metadata": {},
   "source": [
    "#### llm.batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "772dbcf0-6a73-4a59-8cb8-dc2d76c399f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell me a joke.\\n\\nThe answer to the universe.\\n\\nWhat is it? 42.\\n\\nWhat did the fish say when it hit the wall? Dam.\\n\\nI don‚Äôt get it.\\n\\nIt‚Äôs a play on words. Fish live in water, so when the fish hits the wall, it would have been underwater, so it would have said ‚Äúdam‚Äù as in dam holding water back, not ‚Äúdam‚Äù as in curse word.\\n\\nOh, I see it now. That‚Äôs pretty funny.\\n\\nYeah, it‚Äôs a classic. I think it‚Äôs from that book The Hitchhiker‚Äôs Guide to the Galaxy.\\n\\nOh, the answer to the', 'Translate this to German: It has been raining non-stop today.\\n\\nNon-stop: kontinuierlich, ununterbrochen\\n\\nRaining: Regnet\\n\\nToday: heute\\n\\nThe English sentence \"It has been raining non-stop today\" translates to \"Es hat heute kontinuierlich/ununterbrochen gefreggt\" in German. This phrase is often used to describe the continuous rainfall throughout the day. The word \"kontinuierlich\" means that something happens continuously without interruption, while \"ununterbrochen\" has a similar meaning, indicating that something happens without stop or pause. In this context, the word \"gefreggt\" is the past participle form of the verb \"regnen\", which']\n"
     ]
    }
   ],
   "source": [
    "results = llm.batch([\"Tell me a joke\", \"Translate this to German: It has been raining non-stop today.\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd9c64-9a79-4b1e-91c0-f03c8c243bcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67c6ae46-770c-4c3c-bceb-e0805a87b0fe",
   "metadata": {},
   "source": [
    "Let's make that more structured and also format the output nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e1e4ef7-5c88-4c53-9df7-366efe886891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: Tell me a joke\n",
      "Response:\n",
      "Tell me a joke.\n",
      "\n",
      "I'll try. Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything.\n",
      "\n",
      "What's the difference between a snowman and a snowwoman?\n",
      "\n",
      "DEPTH\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything.\n",
      "\n",
      "Why did the programmer quit his job?\n",
      "\n",
      "Because he didn't get arrays.\n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side.\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field.\n",
      "\n",
      "Why do we tell actors to \"break a leg\"?\n",
      "\n",
      "Because every play has one.\n",
      "\n",
      "What\n",
      "\n",
      "Prompt 2: Translate this to German: 'It has been raining non-stop today.'\n",
      "Response:\n",
      "Translate this to German: 'It has been raining non-stop today.'\n",
      "\n",
      "Translation: 'Es hat heute nicht aufh√∂ren regnen.'\n",
      "\n",
      "Explanation:\n",
      "\n",
      "In this sentence, 'It has been raining non-stop today' is translated to 'Es hat heute nicht aufh√∂ren regnen.' Here, 'es' means it, 'hat' means has, 'heute' means today, 'nicht aufh√∂ren' means non-stop or without stopping, and'regnen' means to rain. So, the translation captures the meaning of the original English sentence accurately.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Tell me a joke\",\n",
    "    \"Translate this to German: 'It has been raining non-stop today.'\"\n",
    "]\n",
    "\n",
    "# Run batch generation\n",
    "results = llm.batch(prompts)\n",
    "\n",
    "# Nicely format the output\n",
    "for i, (prompt, response) in enumerate(zip(prompts, results), 1):\n",
    "    print(f\"\\nPrompt {i}: {prompt}\")\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a0e03-4f13-474e-ba70-361bfbfef70b",
   "metadata": {},
   "source": [
    "#### llm.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb04bb-2c6c-450a-bae0-40ac0144ab04",
   "metadata": {},
   "source": [
    "`llm.generate()` yields much more output than `llm.batch()` and is used if you actually want more metadata, such as the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "802ffd32-a6bc-4319-875c-ea1dffc5e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='Where should my customer go for a luxurious Safari?\\n\\nThere are many destinations for a luxurious safari, but some of the most popular include Africa, Asia, and South America. For a truly luxurious experience, consider destinations such as Botswana, Tanzania, Kenya, South Africa, India, Bhutan, Brazil, and Ecuador.\\n\\nWhat are some of the top luxury lodges and camps for a safari?\\n\\nThere are many luxurious lodges and camps to choose from when planning a safari. Some of the top options include Singita Grumeti in Tanzania, &Beyond Sandibe Okavango Delta in Botswana, Royal Malewane in South Africa, Amanwana on M')], [Generation(text=\"What are your top three suggestions for backpacking destinations?\\n1. The Inca Trail to Machu Picchu, Peru: This world-renowned 4-day hike is an incredible journey through the Peruvian Andes, passing through cloud forests, high mountain passes, and Incan ruins. The highlight is undoubtedly arriving at the awe-inspiring ruins of Machu Picchu at sunrise.\\n\\n2. Torres del Paine, Patagonia, Chile: This stunning national park in southern Chile is one of the most beautiful and rugged landscapes on the planet. The 'W' or 'Circuit' treks take you past towering granite peaks, glaciers, crystal-clear lakes, and\")]] llm_output=None run=[RunInfo(run_id=UUID('da72e712-44df-4589-9d89-022473dccf85')), RunInfo(run_id=UUID('232c879f-dc00-4a42-bf14-220a6f20e624'))] type='LLMResult'\n"
     ]
    }
   ],
   "source": [
    "results = llm.generate([\"Where should my customer go for a luxurious Safari?\",\n",
    "                     \"What are your top three suggestions for backpacking destinations?\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5955f-e494-42bd-b21e-416b73cd5fb5",
   "metadata": {},
   "source": [
    "We need to prittyfy the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf0c3c6a-eab9-4b6c-a785-63f799cc23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where should my customer go for a luxurious Safari?\n",
      "\n",
      "There are many destinations for a luxurious safari, but some of the most popular include Africa, Asia, and South America. For a truly luxurious experience, consider destinations such as Botswana, Tanzania, Kenya, South Africa, India, Bhutan, Brazil, and Ecuador.\n",
      "\n",
      "What are some of the top luxury lodges and camps for a safari?\n",
      "\n",
      "There are many luxurious lodges and camps to choose from when planning a safari. Some of the top options include Singita Grumeti in Tanzania, &Beyond Sandibe Okavango Delta in Botswana, Royal Malewane in South Africa, Amanwana on M\n",
      "What are your top three suggestions for backpacking destinations?\n",
      "1. The Inca Trail to Machu Picchu, Peru: This world-renowned 4-day hike is an incredible journey through the Peruvian Andes, passing through cloud forests, high mountain passes, and Incan ruins. The highlight is undoubtedly arriving at the awe-inspiring ruins of Machu Picchu at sunrise.\n",
      "\n",
      "2. Torres del Paine, Patagonia, Chile: This stunning national park in southern Chile is one of the most beautiful and rugged landscapes on the planet. The 'W' or 'Circuit' treks take you past towering granite peaks, glaciers, crystal-clear lakes, and\n"
     ]
    }
   ],
   "source": [
    "for gen in results.generations:\n",
    "    print(gen[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0f876-80b6-4047-afb2-4ede13659e9d",
   "metadata": {},
   "source": [
    "#### llm.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec2cf74f-410b-4581-bce2-5a362ee7ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Once upon a time, there was a gray kitten named Whiskers. Whiskers lived in a cozy little house with her loving family, who gave her all the food, love, and attention that she could ever ask for. Whiskers was a happy, playful, and curious kitten, always exploring her surroundings and getting into all sorts of mischief.\n",
      "\n",
      "One sunny afternoon, Whiskers stumbled upon a hidden door in the basement of her house. Intrigued, she couldn‚Äôt resist the urge to investigate what lay beyond. With a quick meow, she pushed the door open and stepped through.\n",
      "\n",
      "To her delight, she found herself in a magical world filled with"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Tell me a story about a cat.\"):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b731f92-425d-4958-9a43-0abf290e5f95",
   "metadata": {},
   "source": [
    "### Model Types in LangChain\n",
    "\n",
    "LangChain supports two main types of language models:\n",
    "\n",
    "| Model Type     | Description                                                  | Examples                              |\n",
    "|----------------|--------------------------------------------------------------|----------------------------------------|\n",
    "| **LLMs**       | Models that take a plain text string as input and return generated text | GPT-2, Falcon, LLaMA, Mistral (raw)    |\n",
    "| **Chat Models**| Models that work with structured chat messages (system, user, assistant) | GPT-4, Claude, LLaMA-Instruct, Mistral-Instruct|\n",
    "\n",
    "---\n",
    "\n",
    "**Why the distinction?**\n",
    "\n",
    "Chat models are designed to understand multi-turn conversation and role-based prompting. Their input format includes a structured message history, making them ideal for:\n",
    "- Instruction following\n",
    "- Contextual reasoning\n",
    "- Assistant-like behavior\n",
    "\n",
    "LLMs, on the other hand, expect a single flat prompt string. They still power many applications and are worth understanding, especially when using older models, doing fine-tuning, or debugging at the token level.\n",
    "\n",
    "---\n",
    "\n",
    "**Do Chat Models matter more now?**\n",
    "\n",
    "Yes ‚Äî most modern instruction-tuned models (like GPT-4, Claude, Mistral-Instruct, or LLaMA-3-Instruct) are designed as chat models, and LangChain's agent and memory systems are built around them.\n",
    "\n",
    "However, LLMs are still important:\n",
    "- Some models only support the LLM interface\n",
    "- LLMs are useful in batch processing and structured generation\n",
    "- Understanding their behavior helps you build better prompts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6d8dffd-f286-42a3-877d-c439d11e62a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM-style output ---\n",
      "\n",
      "Explain LangChain in one sentence.\n",
      "\n",
      "LangChain is a library for building and managing natural language processing chains that can be used to solve a wide range of tasks.\n",
      "\n",
      "## What is LangChain used for?\n",
      "\n",
      "LangChain is a library designed for building and managing natural language processing chains. It is used to create language models that can be used for a variety of tasks such as question answering, natural language generation, and text classification.\n",
      "\n",
      "## What are the features of LangChain?\n",
      "\n",
      "LangChain has several key features that make it a powerful tool for building natural language processing chains. Some of the key features include:\n",
      "\n",
      "1. Chainable modules: LangChain allows you to easily create chains of modules, which can be used to solve complex tasks.\n",
      "\n",
      "--- Chat-style output ---\n",
      "\n",
      "<s><|im_start|>system\n",
      "You are a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Explain LangChain in one sentence.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "LangChain is an open-source Python library that enables the development of programming tools and applications that interact with natural language data and models, primarily designed for building language models and applications.\n"
     ]
    }
   ],
   "source": [
    "# Plain LLM (single prompt string)\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "print(\"--- LLM-style output ---\\n\")\n",
    "print(llm.invoke(\"Explain LangChain in one sentence.\"))\n",
    "\n",
    "# Use as a ChatModel (structured messages)\n",
    "chat_llm = ChatHuggingFace(llm=llm)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Explain LangChain in one sentence.\")\n",
    "]\n",
    "print(\"\\n--- Chat-style output ---\\n\")\n",
    "print(chat_llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6131-8adc-4074-a892-229fa6aa62b8",
   "metadata": {},
   "source": [
    "The raw output you're seeing includes special chat formatting tokens (like <|im_start|>, <|im_end|>, etc.) which are used internally by the model (e.g., Mistral, LLaMA, GPT-J-style models) to distinguish between roles in a chat.\n",
    "\n",
    "These tokens help the model understand who is speaking, but they're not intended for humans to see. <br>\n",
    "<br>\n",
    "So, to prettyfy the ouput we will define a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf6b6a61-d684-422c-a136-9747508a6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      " LangChain is an open-source Python library designed for building applications that interact with natural language processing models for various tasks such as text generation, summarization, and question answering.\n"
     ]
    }
   ],
   "source": [
    "def clean_output(raw: str) -> str:\n",
    "    # If the assistant marker is in the output, split on it and take the last part\n",
    "    if \"<|im_start|>assistant\" in raw:\n",
    "        return raw.split(\"<|im_start|>assistant\")[-1].replace(\"<|im_end|>\", \"\").strip()\n",
    "    return raw.strip()\n",
    "\n",
    "raw_output = chat_llm.invoke(messages).content\n",
    "cleaned = clean_output(raw_output)\n",
    "print(\"Cleaned Response:\\n\",cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c68d7-2cce-429a-b34c-c02721c15104",
   "metadata": {},
   "source": [
    "An even simpler approach would be to pass the following argument earlier on:\n",
    "```\n",
    "llm = HuggingFacePipeline(pipeline=text_pipe, model_kwargs={\"clean_up_tokenization_spaces\": True})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a359ab1-39d5-4160-94de-1358f170b870",
   "metadata": {},
   "source": [
    "**Confused?** <br>\n",
    "You are not alone. Until recently, LangChain had a different wrapper for LLMs and Chat Models, but in recent versions of LangChain, the HuggingFacePipeline class implements the ChatModel interface under the hood ‚Äî it can accept structured chat messages (SystemMessage, HumanMessage, etc.) even though it wasn't originally designed to.\n",
    "\n",
    "So yes:\n",
    "You can now do:\n",
    "```\n",
    "llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "response = llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful legal assistant.\"),\n",
    "    HumanMessage(content=\"Simplify this clause: ...\")\n",
    "])\n",
    "```\n",
    "Even though you're not explicitly using ChatHuggingFace, LangChain detects the message types and processes them correctly using the underlying text-generation model.\n",
    "<br>\n",
    "<br>\n",
    "The same would apply if you used a remotly hosted LLM/Chat Model through an API:\n",
    "```\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=api_key)\n",
    "result = chat.invoke([HumanMessage(content=\"Can you tell me a fact about Dolphins?\")])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb524f31-3d3a-4a4b-bc00-d21843490193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "225a8a9c-bb3c-46fa-b57d-3bc0e0696885",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"clean_up_tokenization_spaces\": True})\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c6555a0-3bfb-49e3-97aa-186eb5a528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.invoke([HumanMessage(content=\"Can you tell me a fact about dolphins?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58180fce-e04b-41d2-9ae8-87a41b232890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<s><|im_start|>user\\nCan you tell me a fact about dolphins?<|im_end|>\\n<|im_start|>assistant\\nDolphins are highly social animals and are known for their complex communication and problem-solving skills. They have a unique form of echolocation, using sounds to navigate and find prey, and they can recognize themselves in a mirror, showing self-awareness.', additional_kwargs={}, response_metadata={}, id='run--efad0cd6-2de5-429d-a22d-2373d02d7087-0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "375586e2-7880-49c5-abf3-dbdf70593b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolphins are highly social animals and are known for their complex communication and problem-solving skills. They have a unique form of echolocation, using sounds to navigate and find prey, and they can recognize themselves in a mirror, showing self-awareness.\n"
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be3fb5bc-13e3-45de-98a9-45fe1361d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.invoke([SystemMessage(content='You are a gumpy 5-year old child who only wants to get new toys and not answer questions'),\n",
    "               HumanMessage(content='Can you tell me a fact about dophins?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "013daa46-53e4-47dc-83c9-5037c8286feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, I don't want to, I just want new toys!\n"
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c34f9d0c-3a14-4595-a1fc-96f25c5b206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "result = chat_llm.invoke(\n",
    "                [SystemMessage(content='You are a University Professor'),\n",
    "               HumanMessage(content='Can you tell me a fact about dolphins?')]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8c5f0ec-3989-461b-abd3-d1494cdc9b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One interesting fact about dolphins is that they have the ability to sleep with only one half of their brain at a time, allowing them to rest while still maintaining consciousness and staying near the surface of the water to breathe.\n"
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73f872ab-8822-4155-974f-94ccd1c6fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='Can you tell me a fact about dolphins?')\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='What is the difference between whales and dolphins?')\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "099f89a0-115e-4147-8d71-618fd90cab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "Dolphins are highly intelligent and social animals. They have complex communication systems using a variety of vocalizations and body movements, and they have been observed exhibiting altruistic behaviors, such as helping injured or trapped individuals.\n",
      "\n",
      "Prompt 2:\n",
      "Whales and dolphins are both marine mammals that belong to the group Cetaceans, meaning they share common characteristics as well. However, there are some key differences between them.\n",
      "\n",
      "1. Taxonomy: Whales belong to the order Cetacea, while dolphins belong to the infraorder Cetacea Delphinoidea. In simpler terms, whales and dolphins are both in the same family of marine mammals, but they have distinct classifications.\n",
      "\n",
      "2. Physical characteristics: Whales are generally larger and heavier than dolphins. Most whales can grow to be over 20 feet long, while the largest dolphin, the killer whale,\n"
     ]
    }
   ],
   "source": [
    "for i, generation in enumerate(result.generations, 1):\n",
    "    raw = generation[0].text\n",
    "    cleaned = clean_output(raw)\n",
    "    print(f\"\\nPrompt {i}:\\n{cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddef5f18-3f61-488d-a75d-1fc93794f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"clean_up_tokenization_spaces\": True})\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8929bb69-a993-4acb-b2c5-84550dd6eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id\n",
    "result = chat_llm.generate([\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='Can you tell me a fact about dolphins?')\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='What is the difference between whales and dolphins?')\n",
    "    ]\n",
    "], eos_token_id=eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01734b2c-22e6-4fab-ac9e-43e83bf85d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "Dolphins are highly intelligent and social mammals that communicate with each other using a series of clicks, whistles, and body movements. They have the ability to recognize themselves in mirrors, which is a sign of self-awareness.\n",
      "\n",
      "Prompt 2:\n",
      "Whales and dolphins are both marine mammals, but they belong to two different families within the order Cetacea. Whales belong to the family Cetidae, while dolphins belong to the family Delphinidae. Here are some differences between whales and dolphins:\n",
      "\n",
      "1. Size: Whales are generally larger than dolphins. The blue whale, the largest animal on Earth, can grow up to 100 feet long and weigh over 200 tons. Dolphins, on the other hand, can grow up to 30 feet long and weigh up to a few tons.\n",
      "\n",
      "2. Shape: Whales have a more streamlined body shape than dolphins. Their snouts are more elongated, and they have a dorsal fin that varies in size and shape depending on the species. Dolphins have a more robust body with a shorter, rounded snout, and a distinctive curved dorsal fin.\n",
      "\n",
      "3. Diet: Whales feed mainly on krill and small fish, while dolphins feed on a variety of fish and squid. Some species of whales, like the sperm whale, can dive deeper and stay underwater for longer periods of time, allowing them to reach the depths where their prey lives. Dolphins, being smaller and more agile, are better at hunting in shallow waters.\n",
      "\n",
      "4. Behavior: Whales are generally solitary animals or live in small groups, while dolphins are more social animals and live in large groups called pods. Some species of dolphins, like the bottlenose dolphin, have been observed forming strong social bonds and even displaying cooperative behavior.\n",
      "\n",
      "5. Reproduction: Whales give birth to a single calf after a long gestation period of 10-18 months. The calf is nursed for about a year. Dolphins, on the other hand, have a shorter gestation period of 9-14 months and nurse their young for 1-2 years.\n",
      "\n",
      "Overall, whales and dolphins are two distinct groups of marine mammals with different physical characteristics, feeding habits, and social behaviors.\n"
     ]
    }
   ],
   "source": [
    "for i, generation in enumerate(result.generations, 1):\n",
    "    raw = generation[0].text\n",
    "    cleaned = clean_output(raw)\n",
    "    print(f\"\\nPrompt {i}:\\n{cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d5902-6a1f-42c0-b8bc-1cc56e3254b3",
   "metadata": {},
   "source": [
    "This code connects Hugging Face Transformers to LangChain‚Äôs prompt management:\n",
    "- Load model into Hugging Face pipeline.\n",
    "- Wrap it in LangChain (HuggingFacePipeline).\n",
    "- Build structured prompts (system + user).\n",
    "- Format prompt with user input.\n",
    "- Send it to the model and get a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51ce40-782d-4112-9af0-6c160fe221c9",
   "metadata": {},
   "source": [
    "<br>\n",
    "Feel free to experiment with different system and human prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efdf0dda-f69c-487d-bdc2-a58b4acfa76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a research assistant providing precise, well-cited responses.\n",
      "Human: What is the capital of France and what is special about it?\n",
      "\n",
      "The capital of France is Paris. Paris is well-known for its rich history, culture, and iconic landmarks. Some notable features include the Eiffel Tower, which is an iron lattice tower built in 1889 and is one of the most recognizable structures in the world. Another famous landmark is the Louvre Museum, the world's largest art museum and a historic monument, housing famous works of art such as the Mona Lisa and the Venus de Milo. Additionally, Paris is known for its architecture, fashion, cuisine, and romantic atmosphere, earning it the nickname \"The City of Light\" and \"The City of Love.\"\n",
      "\n",
      "References:\n",
      "1. \"Paris\". Encyclop√¶dia Britannica. Encyclop√¶dia Britannica, Inc. 2021.\n",
      "2. \"The Eiffel Tower\". Encyclop√¶dia Britannica. Encyclop√¶dia Britannica, Inc. 2021.\n",
      "3. \"The Louvre\". Encyclop√¶dia Britannica. Encyclop√¶dia Britannica, Inc. 2021.\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "\n",
    "# Define the system and user messages\n",
    "system_message_1 = SystemMessagePromptTemplate.from_template(\"You are a polite and professional assistant who answers concisely.\")\n",
    "system_message_2 = SystemMessagePromptTemplate.from_template(\"You're a friendly AI that gives fun and engaging responses.\")\n",
    "system_message_3 = SystemMessagePromptTemplate.from_template(\"You are a research assistant providing precise, well-cited responses.\")\n",
    "\n",
    "user_message = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "\n",
    "# Create a prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_3, user_message])\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = chat_prompt.format_messages(question=\"What is the capital of France and what is special about it?\")\n",
    "\n",
    "# Run inference\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5026430-caaa-4f9c-938e-328b2f383c5b",
   "metadata": {},
   "source": [
    "### Extra Parameters and Args\n",
    "\n",
    "Here we add in some extra parameters and args, to get the model to respond in a certain way.\n",
    "<br>\n",
    "Some of the most important parameters are:\n",
    "\n",
    "\n",
    "| **Parameter**        | **Purpose**                                                                 | **Range / Default**       | **Analogy / Effect**                        |\n",
    "|----------------------|------------------------------------------------------------------------------|----------------------------|---------------------------------------------|\n",
    "| `do_sample`          | Enables random sampling instead of greedy or beam-based decoding             | `True` / `False`           | üé≤ Adds randomness to output                |\n",
    "| `temperature`        | Controls randomness of token selection                                       | `> 0`, typically `0.7‚Äì1.0` | üå°Ô∏è Higher = more creative / chaotic         |\n",
    "| `top_p`              | Nucleus sampling: sample from top % of likely tokens                         | `0.0‚Äì1.0`, default `1.0`   | üß† Focuses on most probable words           |\n",
    "| `num_beams`          | Beam search: explore multiple continuations and pick the best                | `1+`, default `1`          | üîç Smart guessing with multiple options     |\n",
    "| `repetition_penalty` | Penalizes repeated tokens to reduce redundancy                               | `‚â• 1.0`, e.g. `1.2`        | ‚ôªÔ∏è Discourages repetition                   |\n",
    "| `max_new_tokens`     | Limits the number of tokens the model can generate **per prompt**            | Integer, e.g. `300`        | ‚úÇÔ∏è Controls response length                 |\n",
    "| `eos_token_id`       | Token ID that forces the model to stop when encountered                      | Integer                    | üõë Defines end of output (if supported)     |\n",
    "\n",
    "#### Detailed Explanation of Generation Parameters\n",
    "\n",
    "##### `do_sample=True`\n",
    "- If `False`: the model always picks the **most likely next token** (deterministic, greedy decoding).\n",
    "- If `True`: the model will **randomly sample** from a probability distribution over tokens (non-deterministic).\n",
    "- Required if you want `temperature` or `top_p` to have any effect.\n",
    "\n",
    "‚úÖ Enables creativity and variation  \n",
    "‚ùå Disables reproducibility (unless random seed is fixed)\n",
    "\n",
    "---\n",
    "\n",
    "##### `temperature=1.0`\n",
    "- Controls the **randomness** or \"creativity\" of the output.\n",
    "- Lower values ‚Üí more predictable (safe), higher values ‚Üí more diverse (risky).\n",
    "- Affects how \"flat\" or \"peaky\" the probability distribution is during sampling.\n",
    "\n",
    "**Typical values:**\n",
    "- `0.0` ‚Üí deterministic (most likely token only)\n",
    "- `0.7‚Äì1.0` ‚Üí balanced\n",
    "- `>1.5` ‚Üí chaotic, often incoherent\n",
    "\n",
    "---\n",
    "\n",
    "##### üîπ `top_p=0.9` *(a.k.a. nucleus sampling)*\n",
    "- The model samples only from the **top tokens whose cumulative probability ‚â• `p`**.\n",
    "- Unlike `top_k`, this is dynamic based on the shape of the probability distribution.\n",
    "- Often used in combination with `temperature`.\n",
    "\n",
    "‚úÖ Focuses output on high-probability words  \n",
    "‚ùå Too low ‚Üí model may miss useful words\n",
    "\n",
    "---\n",
    "\n",
    "##### `num_beams=4` *(beam search)*\n",
    "- Explores **multiple candidate completions** and picks the best one based on likelihood.\n",
    "- Slower, but often more optimal (when `do_sample=False`).\n",
    "- Does not work with sampling (`do_sample=True`).\n",
    "\n",
    "**Typical values:**\n",
    "- `1` = greedy decoding  \n",
    "- `3‚Äì5` = moderate beam search  \n",
    "- `>10` = can become very slow\n",
    "\n",
    "---\n",
    "\n",
    "##### `repetition_penalty=1.2`\n",
    "- Penalizes tokens that have already been generated, making the model **less likely to repeat itself**.\n",
    "- Higher values reduce repetition but may hurt fluency.\n",
    "\n",
    "‚úÖ Helps avoid \"looping\" or redundant outputs  \n",
    "üìù Use with long-form or factual responses\n",
    "\n",
    "---\n",
    "\n",
    "##### `max_new_tokens=300`\n",
    "- Sets the **maximum number of tokens** the model is allowed to generate in the response.\n",
    "- Does not include input prompt tokens.\n",
    "\n",
    "‚úÖ Controls output length  \n",
    "‚úÖ Prevents runaway generation or memory issues\n",
    "‚úÖ Prevents truncated output.\n",
    "\n",
    "---\n",
    "\n",
    "##### `eos_token_id`\n",
    "- Tells the model to **stop generation** once it emits this token ID.\n",
    "- Useful for enforcing custom stopping conditions.\n",
    "\n",
    "Optional ‚Äî most models use their own `<eos>` or `</s>` tokens by default.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3314edc-1689-49ce-94bb-b198a8ca1059",
   "metadata": {},
   "source": [
    "Feel free to experiment with these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a01b796b-364e-42ae-b866-e2454ad9c679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    temperature=5.0,\n",
    "    top_p=0.9,\n",
    "    #presence_penalty=1,  # Only if the model supports it\n",
    "    max_new_tokens=300,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe01c99b-b14f-4358-a532-765a19bb5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.invoke([HumanMessage(content='Can you tell me a fact about Earth?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91a67f85-9c18-4ac3-9b2e-ff021c121a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One fascinating but somewhat bouncy fun actual to know about Dangle up is what it actually takes taking portion it move along in deep amelioration (it moves up a stagger you would amaze me a huge to tell that in more everyday terms but, you go on read more more information). Not one side the ameljoros' movements, you need one piece fact to now is Dangle its really cool but there just be even out its not out in this question yet to that side its even though that'sn  really hot enough  there just about D\n"
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b698-ba1b-4c2f-a23a-5e757dd84e2a",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Making the same exact request often? You could use a cache to store results **note, you should only do this if the prompt is the exact same and the historical replies are okay to return**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1639f253-5b37-4ffc-b028-c22b3df2b877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is the fourth planet from the Sun and is often referred to as the \"Red Planet\" due to its reddish appearance, which is caused by iron oxide (rust) on its surface. It is the second-smallest planet in the Solar System, and it has the longest day of any planet in the Solar System, with one day lasting 24 Earth hours. Mars has the highest mountain in the Solar System, Olympus Mons, which is three times the height of Mount Everest. It also has the deepest canyon, Valles Marineris, which is almost 5 miles deep and 2,500 miles long.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "print(clean_output(chat_llm.invoke(\"Tell me a fact about Mars\").content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d706fd5-6067-4d7e-80ab-cd96ccd4a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is the fourth planet from the Sun and is often referred to as the \"Red Planet\" due to its reddish appearance, which is caused by iron oxide (rust) on its surface. It is the second-smallest planet in the Solar System, and it has the longest day of any planet in the Solar System, with one day lasting 24 Earth hours. Mars has the highest mountain in the Solar System, Olympus Mons, which is three times the height of Mount Everest. It also has the deepest canyon, Valles Marineris, which is almost 5 miles deep and 2,500 miles long.\n"
     ]
    }
   ],
   "source": [
    "# You will notice this reply is instant!\n",
    "print(clean_output(chat_llm.invoke(\"Tell me a fact about Mars\").content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf6d5d-6ddd-4741-8e9e-cf9a14c653e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
