{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cfbde93-696b-42eb-a709-38ea466560f3",
   "metadata": {},
   "source": [
    "# Evaluation Methods and Metrics for LLMs\n",
    "\n",
    "Evaluation metrics are essential for measuring how well language models perform various tasks, such as text generation, classification, and translation. We will cover some of the most commonly used metrics: perplexity, accuracy, BLEU, ROUGE, and METEOR. You need to choose the metric that is most suited to the ues-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8ae21-a548-41fa-b96c-fb461e72e7e9",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall & F1-Score\n",
    "All of these metrics are typically used for classification tasks.  \n",
    "\n",
    "* **Accuracy** measures how many predictions made by the model are correct out of the total predictions. $$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "* **Precision** measures how many of the predicted positive labels were actually correct. $$\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "* **Recall** measures how many of the actual positive labels were correctly predicted. $$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "* **F1-score** is the harmonic mean of precision and recall, providing a single measure that balances both. $$\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a8264-0558-42e3-83c6-310f7a2c3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bf56a-4a4a-468d-b1ad-507c9c1bf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a classification dataset\n",
    "dataset = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/stanfordnlp--imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11c616-24aa-49fd-b4ce-d91a9bd79fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = evaluate.load(\"accuracy\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b382a-3687-45dc-9969-8141989356d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\", trust_remote_code=True)\n",
    "recall =evaluate.load(\"recall\", trust_remote_code=True)\n",
    "f1 = evaluate.load(\"f1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687f359-a7eb-4dbe-91eb-dc74d920bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(preds, labels):\n",
    "    print(\"Accuracy: \",accuracy.compute(predictions=preds, references=labels),\n",
    "         \"\\nPrecision: \",precision.compute(predictions=preds, references=labels),\n",
    "         \"\\nRecall: \",recall.compute(predictions=preds, references=labels),\n",
    "         \"\\nF1-score: \",f1.compute(predictions=preds, references=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4abd1-64e2-4a08-8ea9-9c25e36e5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example predictions (replace with actual predictions from your model)\n",
    "preds = [0, 1, 0, 1]\n",
    "labels = [0, 1, 1, 1]\n",
    "compute_scores(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6c843-c9be-4d38-88e2-327f2a304de4",
   "metadata": {},
   "source": [
    "## BLEU, ROUGE\n",
    "Measuring performance on a text generation task is not as easy as with standard classification tasks such as sentiment analysis or named entity recognition. Take the example of translation; given a sentence like “I love dogs!” in English and translating it to Spanish there can be multiple valid possibilities, like “¡Me encantan los perros!” or “¡Me gustan los perros!” Simply checking for an exact match to a reference translation is not optimal; even humans would fare badly on such a metric because we all write text slightly differently from each other (and even from ourselves, depending on the time of the day or year!). Fortunately, there are alternatives.\n",
    "\n",
    "Two of the most common metrics used to evaluate generated text are BLEU and ROUGE. Let’s take a look at what they do.\n",
    "\n",
    "### BLEU\n",
    "BLEU is a widely used metric, especially for machine translation. The idea of BLEU is to compare words or n-grams. It's is a precision-based metric, which means that when we compare the two texts we count the number of words in the generation that occur in the reference and divide it by the length of the generation. Because the precision score favours short generations, we need to compensate for that with the brevity penalty. One of the limitations of this metric is that it doesn’t take synonyms into account.  \n",
    "This is the formula of the BLEU score:\n",
    "$$\\text{BLEU-N} = BR \\times \\left( \\prod_{n=1}^{N} p_n \\right)^{\\frac{1}{N}}$$\n",
    "\n",
    "The `bleu_metric` object is an instance of the `Metric` class, and works like an aggregator: you can add single instances with `add()` or whole batches via `add_batch()`. Once you have added all the samples you need to evaluate, you then call `compute()` and the metric is calculated. This returns a dictionary with several values, such as the precision for each n-gram, the length penalty, as well as the final BLEU score. Let’s look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3340b7f-7d64-4297-b22b-5747b97cf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"sacrebleu\") # sacrebleu doesn't expect the text to be tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cf1d1-3cd5-4768-9884-270d1405115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric.add(\n",
    "    prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute()\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2fc73-6021-4607-a01f-191e040aad24",
   "metadata": {},
   "source": [
    "Lets see what the less obvious metrics mean here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93021a-7dc2-494d-8776-4035bb718be9",
   "metadata": {},
   "source": [
    "#### Counts:\n",
    "The counts represent the number of n-grams (1-gram, 2-gram, etc.) in the prediction that also appear in the reference. These are the \"matches\" found between the predicted and reference text.  \n",
    "  \n",
    "- 1-gram counts: The number of single words in the prediction that are present in the reference.\n",
    "- 2-gram counts: The number of consecutive pairs of words (bigrams) in the prediction that are present in the reference.\n",
    "- ...\n",
    "\n",
    "#### Totals:\n",
    "The totals represent the total number of n-grams in the prediction (regardless of whether they match the reference).\n",
    "\n",
    "\n",
    "- 1-gram totals: The total number of individual words in the prediction.\n",
    "- 2-gram totals: The total number of consecutive pairs of words (bigrams) in the prediction.\n",
    "- ...\n",
    "\n",
    "#### Precisions:\n",
    "The precision for each n-gram level (1-gram, 2-gram, etc.) is calculated as the ratio between counts (matched n-grams) and totals (total n-grams in the prediction).\n",
    "\n",
    "#### Brevity Penalty (BP):\n",
    "This penalizes short predictions compared to the reference length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3df4b-b26c-488e-9e81-85794f6c9afe",
   "metadata": {},
   "source": [
    "We can see the precision of the 1-gram is 2/6, whereas the precisions for the 2/3/4-grams are all 0. The overall score should become 0, but `bleu_metric`applies some smoothing, so the score doesn't drop to 0 just because one n-gram gets 0 precision. If you would like to get the exact value accoding to the formula you need to use `results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4662032-c7d0-4f7f-b4c1-cff2a3ba294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric.add(\n",
    "    prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute()\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7788b7c-0046-4681-8cfb-2a9598b951db",
   "metadata": {},
   "source": [
    "We observe that the precision scores are much better. The 1-grams in the prediction all match, and only in the precision scores do we see that something is off. For the 4-gram there are only two candidates, `[\"the\", \"cat\", \"is\", \"on\"]` and `[\"cat\", \"is\", \"on\", \"mat\"]`, where the last one does not match, hence the precision of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b0663-a8c0-4c49-936c-f68feedb7ed5",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "The ROUGE score was specifically developed for applications like summarization where high recall is more important than just precision. The approach is very similar to the BLEU score in that we look at different n-grams and compare their occurrences in the generated text and the reference texts. The difference is that with ROUGE we check how many n-grams in the reference text also occur in the generated text.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a4114-1456-4f1c-b6d1-0ad44587d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fd57e-0dee-41d7-bf0f-7d9e0f331ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/abisee--cnn_dailymail\", \"3.0.0\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2aa13-9df1-4276-9c58-68aee1d99958",
   "metadata": {},
   "source": [
    "The dataset has three columns: article, which contains the news articles, highlights with the summaries, and id to uniquely identify each article. Let’s look at an excerpt from an article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cf0a5-5863-485a-8592-670a5e248efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8171a9c-980a-43a3-a6b8-d9371a247f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8eaeb9-3c83-4e8b-9a0c-fd6a1ee02ee8",
   "metadata": {},
   "source": [
    "A convention in summarization is to separate the summary sentences by a newline. We could add a newline token after each full stop, but this simple heuristic would fail for strings like “U.S.” or “U.N.” The Natural Language Toolkit (NLTK) package includes a more sophisticated algorithm that can differentiate the end of a sentence from punctuation that occurs in abbreviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92517db9-a64f-463b-8c40-45173c65f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b30490-ae19-4342-9c47-a0bf58599839",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length=512, truncation=True, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(\n",
    "    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498d293-3419-49b7-9cc5-ee675a033da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec937b-a1d8-482f-9eaa-ce3ec0677140",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []  # Initialize an empty list to store results\n",
    "\n",
    "# Loop over each model name and add prediction-reference pairs\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()  # Compute after adding each model's prediction\n",
    "\n",
    "    # Collect f-measure scores directly for each ROUGE metric\n",
    "    rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "# Create the DataFrame using model names as the index\n",
    "df = pd.DataFrame.from_records(records, index=summaries.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c2b8d-f998-44b5-bda4-07986dfa7d0b",
   "metadata": {},
   "source": [
    "In the Hugging Face Datasets implementation, two variations of ROUGE are calculated: one calculates the score per sentence and averages it for the summaries (ROUGE-L), and the other calculates it directly over the whole summary (ROUGE-Lsum). ROUGE-1 refers to the overlap of unigrams (single words) between the system-generated summary and the reference summary. ROUGE-2 refers to the overlap of bigrams (two consecutive words) between the system-generated summary and the reference summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbce880-5f2c-4ec9-bbfc-63eead94ee6b",
   "metadata": {},
   "source": [
    "### Human Evaluation\n",
    "Human evaluation may be necessary for tasks like summarization, where metrics like BLEU and ROUGE may not fully capture the quality of the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06faccc4-76ce-42de-b6e0-f32f17e8285a",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "Perplexity is a commonly used metric for evaluating language models, particularly those involved in tasks like text generation, machine translation, or language modeling. It measures how well a language model predicts a sample of text and is directly related to the probability assigned by the model to the test data. Or in other words perplexity quantifies how uncertain or \"perplexed\" the model is about the next word in a sequence. A lower perplexity indicates that the model is better at predicting the next word. A perplexity close to 1 indicates perfect predictions. A higher perplexity means the model is more \"confused.\" So, if a model has a perplexity of 10, this can be interpreted as the model being as uncertain as if it were choosing the next word from a set of 10 equally likely possibilities.\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i)\\right)$$\n",
    "Hugging Face does not provide a built in perplexity metric like we know it from accuracy, BLEU, or ROUGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cee232-8028-415c-a717-03eb1e72acb4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook shows how to evaluate different aspects of an LLM using multiple metrics. Each task may require different metrics depending on the output format and objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9fa1b-d0c3-4f70-896d-3bf3dc174635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
