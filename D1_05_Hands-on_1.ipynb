{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43def0fd-3fba-4e39-a4bb-66a413b888aa",
   "metadata": {},
   "source": [
    "## Hands-On Example: Applying What You've Learned\n",
    "\n",
    "In this notebook, you will apply the concepts covered in the previous sessions, including:\n",
    "\n",
    "1. Understanding the Huggingface Ecosystem\n",
    "2. Working with Transformer models\n",
    "3. Implementing Tokenization and Embeddings\n",
    "4. Utilizing a pre-trained model for a NLP task\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Fine-tune a pre-trained Transformer model (e.g., BERT) on a text classification task (sentiment analysis using the IMDB dataset). During this exercise, you will:\n",
    "- Load and preprocess the dataset.\n",
    "- Tokenize the input data.\n",
    "- Apply a pre-trained model to extract embeddings.\n",
    "- Fine-tune the model using memory-efficient techniques.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df24af-1868-4273-9c88-10095509a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe31d9-ee97-4cde-829b-31328db38fb5",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "We will use the IMDB dataset for binary sentiment classification. The goal is to classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a534f50-5960-4e67-ba9e-2004eda2ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/stanfordnlp--imdb\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"Sample from the IMDB dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64767097-7870-481e-8059-e4eb1f56f740",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "Use a pre-trained tokenizer to convert the text data into token IDs that the model can understand. We will use the BERT tokenizer for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e6c8e-9ac6-4ba3-936e-9b6d1af7ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\")\n",
    "\n",
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6b225-d516-4df0-ac20-7fe8d8cc7d05",
   "metadata": {},
   "source": [
    "## Load a Pre-trained Model\n",
    "\n",
    "Now, let's load a pre-trained BERT model for sequence classification. This model will be fine-tuned on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e02f04-a41c-4cc3-b7da-2a484f7dc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72aaf5-9218-4c68-9c69-e8e8e3f6d2a5",
   "metadata": {},
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "Set up the training arguments for fine-tuning the model. This includes the output directory, batch size, number of epochs, and logging settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c015d79-07e6-49f0-afbf-43b2afcc0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94393b-354a-4897-8d58-5305b2acceb4",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model\n",
    "\n",
    "Use the Huggingface `Trainer` API to fine-tune the model on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfa783-df99-4eff-825e-4ee54e7b5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c3030-6467-4f95-9fbf-18b9f8619e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b2c3b-00ea-4e71-80cb-c8b39c84307a",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Evaluate the model's performance on the test set to understand its accuracy and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc2602-4912-4c0d-9548-cd875d57a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385187d0-26fb-4e0f-842d-406b428d97d5",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Fine-Tune Another Model\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Choose a different pre-trained model from the Huggingface Hub (e.g., \"distilbert-base-uncased\").\n",
    "2. Load and tokenize the dataset.\n",
    "3. Configure training arguments.\n",
    "4. Fine-tune the model.\n",
    "5. Evaluate its performance.\n",
    "\n",
    "**Questions to Consider:**\n",
    "\n",
    "- How does the model's performance compare to BERT?\n",
    "- What are the differences in memory usage between models?\n",
    "- What steps can be taken to optimize memory usage further?\n",
    "\n",
    "Try it out below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede54b82-9412-438b-991c-cf870df76798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose a different model\n",
    "model_dir = \"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/distilbert--distilbert-base-uncased\"  # Change to any other pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Step 2: Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Step 3: Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Step 4: Fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results for the new model:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539397e2-bdb7-4423-b8e8-12429e450022",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this hands-on example, you applied the concepts learned in previous sessions to fine-tune a pre-trained Transformer model on a text classification task. You practiced loading datasets, tokenizing text, and configuring training arguments to achieve optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182e3e4-8a8a-41ab-952f-9dead82ac27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a374b-e8f4-4a4e-9122-ad2376a2d877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
