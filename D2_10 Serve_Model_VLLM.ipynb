{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68bcd6-bca8-4376-b231-0c8c84c532e4",
   "metadata": {},
   "source": [
    "## Load a model with VLLM and serve an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96197c4-9bdf-4895-a209-f97a30660b66",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to load a model (we are using the bitsandbytes quantized merged model from the previous notebook) and serve it for inference using VLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0e4a77-b509-4e9a-8b44-d735c8539ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_vllm.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=boost_usr_prod\n",
    "# #SBATCH --qos=boost_qos_dbg\n",
    "#SBATCH --account=EUHPC_D20_063\n",
    "#SBATCH --reservation=s_tra_ncc\n",
    "\n",
    "## Specify resources:\n",
    "## Leonardo Booster: 32 CPU cores and 4 GPUs per node => request 8 * number of GPUs CPU cores\n",
    "## Leonardo Booster: 512 GB in total => request approx. 120 GB * number of GPUs requested\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=1  # up to 4 on Leonardo\n",
    "#SBATCH --ntasks-per-node=1  # always 1\n",
    "#SBATCH --mem=120GB  # should be 120GB * gpus-per-task on Leonardo\n",
    "#SBATCH --cpus-per-task=8  # should be 8 * gpus-per-task on Leonardo\n",
    "\n",
    "#SBATCH --time=0:10:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load anaconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate /leonardo/pub/userexternal/mpfister/conda_env_martin29\n",
    "\n",
    "# Run AI scripts:\n",
    "export OMP_NUM_THREADS=32\n",
    "export PORT=$((RANDOM % (60000 - 8000) + 8000))  # Choose a random port\n",
    "echo \"VLLM will serve an OpenAI compatible AI at http://$HOSTNAME:$PORT/v1\"\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model output/phi-3.5-mini-instruct-guanaco-fsdp_merged_bnb \\\n",
    "    --served-model-name finetuned-phi-3.5-mini \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 4096 \\\n",
    "    --api-key our-secret-api-key \\\n",
    "    --port $PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f8e6-6855-491d-86dd-46de21d4d753",
   "metadata": {},
   "source": [
    "Now submit the SLURM job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaf7a89-88ee-45f7-9fd0-fb5820db25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16603457\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_vllm.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ddfe-44de-4b68-aab8-00f08a610e8c",
   "metadata": {},
   "source": [
    "Execute `squeue` to see, if your job is already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e10a89e-425e-4570-993b-c595869183e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          16603457 boost_usr run_vllm mpfister  R       0:13      1 lrdn1935\n",
      "          16599181 boost_usr jupyterl mpfister  R    2:24:23      1 lrdn2012\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072291e5-fea9-4821-9958-2da0b0ada7b5",
   "metadata": {},
   "source": [
    "Once your job is running, look at the output of the job using the following command (replace the number with the JOBID from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d325f17-2e5c-4b26-82d4-98e5b4f52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLM will serve an OpenAI compatible AI at http://lrdn1935:12520/v1\n",
      "INFO 06-10 19:31:13 [__init__.py:243] Automatically detected platform cuda.\n",
      "INFO 06-10 19:31:16 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-10 19:31:16 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-10 19:31:16 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-10 19:31:17 [api_server.py:1289] vLLM API server version 0.9.0.1\n",
      "INFO 06-10 19:31:18 [cli_args.py:300] non-default args: {'port': 12520, 'api_key': 'our-secret-api-key', 'model': 'output/phi-3.5-mini-instruct-guanaco-fsdp_merged_bnb', 'dtype': 'bfloat16', 'max_model_len': 4096, 'served_model_name': ['finetuned-phi-3.5-mini']}\n",
      "INFO 06-10 19:31:18 [config.py:213] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 06-10 19:31:31 [config.py:793] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 06-10 19:31:32 [config.py:907] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 06-10 19:31:32 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 06-10 19:31:40 [__init__.py:243] Automatically detected platform cuda.\n",
      "INFO 06-10 19:31:45 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-10 19:31:45 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-10 19:31:45 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-10 19:31:45 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-10 19:31:45 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='output/phi-3.5-mini-instruct-guanaco-fsdp_merged_bnb', speculative_config=None, tokenizer='output/phi-3.5-mini-instruct-guanaco-fsdp_merged_bnb', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=finetuned-phi-3.5-mini, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-10 19:31:46 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1504c92be550>\n",
      "INFO 06-10 19:31:46 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-10 19:31:46 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-10 19:31:46 [gpu_model_runner.py:1531] Starting to load model output/phi-3.5-mini-instruct-guanaco-fsdp_merged_bnb...\n",
      "INFO 06-10 19:31:47 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-10 19:31:47 [backends.py:35] Using InductorAdaptor\n",
      "INFO 06-10 19:31:47 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.12it/s]\n",
      "\n",
      "INFO 06-10 19:31:49 [gpu_model_runner.py:1549] Model loading took 2.3205 GiB and 2.573176 seconds\n",
      "INFO 06-10 19:31:56 [backends.py:459] Using cache directory: /leonardo/home/userexternal/mpfister/one-click-access-home/.cache/vllm/torch_compile_cache/a8c04a0f83/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-10 19:31:56 [backends.py:469] Dynamo bytecode transform time: 6.77 s\n",
      "INFO 06-10 19:32:00 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 3.879 s\n",
      "INFO 06-10 19:32:02 [monitor.py:33] torch.compile takes 6.77 s in total\n",
      "INFO 06-10 19:32:03 [kv_cache_utils.py:637] GPU KV cache size: 146,880 tokens\n",
      "INFO 06-10 19:32:03 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 35.86x\n",
      "INFO 06-10 19:32:27 [gpu_model_runner.py:1933] Graph capturing finished in 24 secs, took 2.83 GiB\n",
      "INFO 06-10 19:32:27 [core.py:167] init engine (profile, create kv cache, warmup model) took 37.63 seconds\n",
      "INFO 06-10 19:32:27 [loggers.py:134] vllm cache_config_info with initialization after num_gpu_blocks is: 9180\n",
      "INFO 06-10 19:32:27 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:12520\n",
      "INFO 06-10 19:32:27 [launcher.py:28] Available routes are:\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /health, Methods: GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /load, Methods: GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /ping, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /ping, Methods: GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /tokenize, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /detokenize, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/models, Methods: GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /version, Methods: GET\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /pooling, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /classify, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /score, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/score, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /rerank, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /invocations, Methods: POST\n",
      "INFO 06-10 19:32:27 [launcher.py:36] Route: /metrics, Methods: GET\n",
      "INFO:     Started server process [644656]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-16603457.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3db6b1-d720-4029-abd7-ba7c214e8687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48eab65a-9aa1-40d4-9ba9-c2744e492aa6",
   "metadata": {},
   "source": [
    "Now we can access the OpenAI-compatible API using any other software. For example, we can use the `openai` python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f64792a-9a91-40f4-a115-a3675d0d8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response:\n",
      " ChatCompletion(id='chatcmpl-e91290ff6aee49a988f0b4a2c51585c4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\" Sure! Here's a joke for you:\\n\\nWhy don't skeletons fight each other?\\n\\nBecause they don't have the guts to.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=32007)], created=1749576851, model='finetuned-phi-3.5-mini', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=17, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None, kv_transfer_params=None)\n",
      "\n",
      "Answer from the chatbot:  Sure! Here's a joke for you:\n",
      "\n",
      "Why don't skeletons fight each other?\n",
      "\n",
      "Because they don't have the guts to.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"our-secret-api-key\"\n",
    "# TODO: Edit the following line to use the URL from the output of your own VLLM job\n",
    "openai_api_base = \"http://lrdn1935:12520/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"finetuned-phi-3.5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print('Chat response:\\n', chat_response)\n",
    "print('')\n",
    "print('Answer from the chatbot:', chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67826641-2cb6-45e9-b220-8a76eb1c7f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec75488-c193-4b7c-8bba-166826672e7e",
   "metadata": {},
   "source": [
    "Finally, cancel your VLLM job to free up the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df32c4f-238c-46e1-9375-2582ea138c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 16603457"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c020f6-e785-41f6-bb49-58d56994389c",
   "metadata": {},
   "source": [
    "If you want to, you can also delete the files that we create above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cc3e8b-6303-46d4-9051-f3551e527e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm run_vllm.slurm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049395-b496-4412-abb7-1c5a7f592cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
