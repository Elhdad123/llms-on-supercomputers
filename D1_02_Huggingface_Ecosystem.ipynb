{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869f918b-2554-4152-857b-1c38a3e12522",
   "metadata": {},
   "source": [
    "# Huggingface Ecosystem Overview\n",
    "\n",
    "Huggingface is a company that has revolutionized the field of Natural Language Processing (NLP) by providing an open-source library and tools that facilitate the use of state-of-the-art models for various NLP tasks.\n",
    "\n",
    "### Key Components of the Huggingface Ecosystem:\n",
    "\n",
    "1. **Transformers**: A library that provides APIs and tools to easily download and fine-tune state-of-the-art pre-trained models.\n",
    "2. **Datasets**: A library to access and process large datasets used for NLP and other machine learning tasks.\n",
    "3. **PEFT (Parameter-Efficient Fine-Tuning)**: A library for efficient model fine-tuning using parameter-efficient techniques.\n",
    "4. **Accelerate**: A library to accelerate PyTorch and TensorFlow models' training and deployment across multiple devices (GPUs).\n",
    "5. **Huggingface Hub**: A central repository for pre-trained models, datasets, and metrics, allowing seamless sharing and collaboration.\n",
    "\n",
    "Let's explore each component in detail with hands-on examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4c4d1-370f-4c16-a565-b4d5fd8ebce4",
   "metadata": {},
   "source": [
    "## Transformers Library\n",
    "\n",
    "The `transformers` library by Huggingface is a powerful toolkit that provides state-of-the-art pre-trained models and easy-to-use APIs for NLP tasks such as text classification, named entity recognition, translation, text generation, and more.\n",
    "\n",
    "### Key Features:\n",
    "- Provides thousands of pre-trained models.\n",
    "- Supports multiple frameworks: PyTorch, TensorFlow, and JAX.\n",
    "- Easy integration with the Huggingface Hub.\n",
    "\n",
    "### Example: Loading a Pre-trained Model and Running Inference\n",
    "Let's load a pre-trained BERT model and use it for a simple text classification task.\n",
    "For that we use `pipeline()`.\n",
    "`pipeline()` is a very convenient way to use a pretrained model for inference. You can use the `pipeline()` out-of-the-box for many tasks across different modalities, as can be seen [here](https://huggingface.co/docs/transformers/quicktour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7e310-283a-4f50-abc9-95b48754a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment-analysis pipeline\n",
    "# classifier = pipeline(\"sentiment-analysis\") This would determine which model should be used for us.\n",
    "# However, I would like to go easy on our disk space available and use model, which I have downloaded in advance to a shared directory:\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/distilbert--distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4fae5-4e0b-4dec-a819-133af6687efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model used: {classifier.model.name_or_path}\\n\")\n",
    "print(classifier.model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8481c1-06ec-43ed-b5fd-7f11de491ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline with some example text\n",
    "result = classifier(\"Huggingface is an amazing platform for NLP research!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bda56d-26b2-4686-b81b-0ea95eb22aeb",
   "metadata": {},
   "source": [
    "The result isn't great. This is because we used a model, which doesn't hasen't been fine-tuned for the task.\n",
    "Let's try this one and see how it works:\n",
    "<br>\n",
    "`/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/distilbert--distilbert-base-uncased-finetuned-sst-2-english`\n",
    "\n",
    "Feel free to try out different sentences, also negative ones.\n",
    "Should you have more than one input, pass them as a list. You will then get a list of dictionaries as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e309aae-5281-4496-a4e0-24a0a04194b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier([\"EuorCC courses are not bad at all. There is lots to gain from them.\",\n",
    "                    \"I think Large Language Models are overrated.\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5b3a0-5abd-439f-bb85-3434364f9299",
   "metadata": {},
   "source": [
    "You can see, that the model not only predicts the sentiment, but also outputs a score, which is a probability that indicates the model's confidence in its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293fb22-5e39-4db2-a423-668547372757",
   "metadata": {},
   "source": [
    "## Datasets Library\n",
    "\n",
    "The `datasets` library provides a lightweight library for easily downloading and using datasets in NLP and other ML domains. It is optimized for both in-memory and out-of-memory (on-disk) use, making it suitable for handling very large datasets.\n",
    "\n",
    "### Key Features:\n",
    "- Access to thousands of datasets in various domains.\n",
    "- Built-in data processing tools such as caching, shuffling, and batching.\n",
    "- Easy integration with the `transformers` library for model training.\n",
    "\n",
    "### Example: Loading and Exploring a Dataset\n",
    "Let's load a sample dataset and explore its content.\n",
    "The **IMDB** dataset is a popular benchmark dataset used for sentiment analysis tasks in natural language processing. It consists of movie reviews from the Internet Movie Database (IMDB) and is specifically designed for binary sentiment classification: determining whether a given movie review expresses a positive or negative sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d7a02-bc94-4b72-81d6-a625a49c4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/stanfordnlp--imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751799cb-ccd6-499d-bf29-45a267224d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 10th example in the training set\n",
    "print(dataset['train'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f351ea-702a-4a65-8780-fe2c3e4b12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebe225-aefa-40a1-8d7f-f8b84fefc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for binary classification (num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477be9ec-41fc-4943-8402-bac5088eb361",
   "metadata": {},
   "source": [
    "The Warning we got is to be expected, because the classification head we added has not been pre-trained and the weights and biases have been newly initialized.\n",
    "\n",
    "The bert-base-uncased model is a general-purpose, pre-trained BERT model. It has been trained on a large corpus of text using self-supervised objectives (like masked language modeling) but not for specific tasks like sentiment analysis, classification, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2956f-8141-4c4b-85b4-4ea896b19a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for efficient fine-tuning\n",
    "config = LoraConfig(r=8)\n",
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65bc53-16a6-4941-8023-f17dcd1d7779",
   "metadata": {},
   "source": [
    "`get_peft_model` is a function from the PEFT library that takes a pre-trained model and a LoRA configuration (`LoraConfig`) and returns a new model that has been adapted for parameter-efficient fine-tuning.\n",
    "The new model (`peft_model`) has the same architecture as the original model (model) but with additional parameters introduced by LoRA that enable efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9af54-809e-49fc-8450-d217dd133a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/stanfordnlp--imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7dc50a-3108-4f81-a0b6-a8bf77ecad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/leonardo_scratch/fast/EUHPC_D20_063/huggingface/models/google--bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41bfd6-e79a-4bda-8484-db7b0bbb7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5999d9c-39a2-46e5-ad17-b38179d80233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format to include PyTorch tensors for input_ids, attention_mask, and label\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")  # Rename 'label' column to 'labels'\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2da5a4-7154-4f74-8240-512d1ec18ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(output_dir=\"./results\",\n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  report_to=\"none\",\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74a845-84df-4547-a8cb-ff049a23d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(model=peft_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_datasets['train'],\n",
    "                  eval_dataset=tokenized_datasets['test'])\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858ddfb-7266-4dc5-b2d1-2e93e2770c91",
   "metadata": {},
   "source": [
    "## Huggingface Hub\n",
    "\n",
    "The Huggingface Hub is a platform for sharing models, datasets, and demos. It allows developers and researchers to collaborate, publish, and discover models and datasets, making the entire community's work more accessible.\n",
    "\n",
    "### Key Features:\n",
    "- A central repository for models, datasets, and metrics.\n",
    "- Tools for versioning, collaboration, and deployment.\n",
    "- Integrated with Huggingface libraries for easy use.\n",
    "\n",
    "### Example: Uploading a Model to the Huggingface Hub\n",
    "Here's how you can upload a model to the Huggingface Hub.\n",
    "\n",
    "``` python\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the API\n",
    "api = HfApi()\n",
    "\n",
    "# Example of creating a new repository (Requires authentication)\n",
    "repo_name = \"my-awesome-model\"\n",
    "api.create_repo(repo_name)\n",
    "\n",
    "# Save model locally and push to hub\n",
    "model.save_pretrained(f\"./{repo_name}\")\n",
    "api.upload_folder(repo_id=repo_name, folder_path=f\"./{repo_name}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04781ed0-9729-47ab-9b64-5f14a0716bc4",
   "metadata": {},
   "source": [
    "### Explore Available Models\n",
    "\n",
    "We will use the Huggingface API to search for available models and filter them based on certain criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36aa4a-1f6a-44ca-a1be-7c7d6342bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203ce2d-9dd6-4a12-bf7e-7628e3d64074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Huggingface API\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5beccaf-410f-4c0e-8728-5fae880b2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for models in the Huggingface Model Hub\n",
    "models = list(api.list_models(limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd92d8-796e-4eaf-b226-4831a4732c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the fetched models\n",
    "for model in models:\n",
    "    print(model.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111f01b-8a5d-4059-9d74-f275b2c16a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 models for demonstration\n",
    "models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b06c4-f6ed-4190-978c-c7de60c33b1d",
   "metadata": {},
   "source": [
    "Should you with to make that visually more pleasing, you can do so, by creating a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fd47d-fac4-4ea6-bf6c-ddbb99b490ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ed909-632a-40b1-aa46-0d21233aa3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame by first creating a dictionary:\n",
    "model_data = []\n",
    "\n",
    "for model in models:\n",
    "    model_info = {\n",
    "        'Model ID': model.modelId,\n",
    "        'Tags': ', '.join(model.tags) if model.tags else 'N/A',\n",
    "        'Downloads': model.downloads,\n",
    "        'Likes': model.likes,\n",
    "        'Pipeline Tag': model.pipeline_tag if model.pipeline_tag else 'N/A',\n",
    "        'Last Modified': model.lastModified.strftime('%Y-%m-%d') if model.lastModified else 'N/A'\n",
    "    }\n",
    "    model_data.append(model_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88db43-e578-4897-9798-801612f8d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the dictionary to pandas DataFrame:\n",
    "df_models = pd.DataFrame(model_data)\n",
    "\n",
    "# Display the first 5 entries of the DataFrame:\n",
    "df_models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54fa57-1f74-4de2-ae98-c914f7413874",
   "metadata": {},
   "source": [
    "#### Analyze Model Information\n",
    "\n",
    "Inspect the models to understand their details, such as architecture, number of parameters, and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e6aeb-6863-4c8a-9d11-40180f8d58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about a specific model\n",
    "model_name = \"bert-base-uncased\"  # Example model\n",
    "model_info = api.model_info(model_name)\n",
    "\n",
    "print(f\"Model: {model_info.modelId}\")\n",
    "print(f\"Description: {model_info.cardData.get('description', 'No description available')}\")\n",
    "print(f\"Framework: {model_info.pipeline_tag}\")\n",
    "print(f\"Tags: {model_info.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9df14e-6292-4df7-afd7-89c2cab531aa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the Huggingface ecosystem, including the `transformers`, `datasets`, and Huggingface Hub.\n",
    "We will get to know `PEFT` and `accelerate` in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518fdea-817b-487e-b882-c6bc942c2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd46d3a-2ab5-4771-a979-6980e17de4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
